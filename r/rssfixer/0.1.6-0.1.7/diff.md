# Comparing `tmp/rssfixer-0.1.6.tar.gz` & `tmp/rssfixer-0.1.7.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "rssfixer-0.1.6.tar", max compression
+gzip compressed data, was "rssfixer-0.1.7.tar", max compression
```

## Comparing `rssfixer-0.1.6.tar` & `rssfixer-0.1.7.tar`

### file list

```diff
@@ -1,6 +1,6 @@
--rw-r--r--   0        0        0     1059 2023-04-30 11:20:03.145292 rssfixer-0.1.6/LICENSE
--rw-r--r--   0        0        0    10493 2023-05-01 06:44:43.555449 rssfixer-0.1.6/README.md
--rw-r--r--   0        0        0     2865 2023-05-01 06:42:19.258554 rssfixer-0.1.6/pyproject.toml
--rw-r--r--   0        0        0       98 2023-04-30 07:34:12.316893 rssfixer-0.1.6/src/rssfixer/__init__.py
--rw-r--r--   0        0        0    12914 2023-04-30 11:38:07.420585 rssfixer-0.1.6/src/rssfixer/rss.py
--rw-r--r--   0        0        0    11143 1970-01-01 00:00:00.000000 rssfixer-0.1.6/PKG-INFO
+-rw-r--r--   0        0        0     1059 2023-05-02 04:17:59.003602 rssfixer-0.1.7/LICENSE
+-rw-r--r--   0        0        0    11067 2023-06-02 08:44:27.393582 rssfixer-0.1.7/README.md
+-rw-r--r--   0        0        0     2828 2023-06-02 08:46:29.988188 rssfixer-0.1.7/pyproject.toml
+-rw-r--r--   0        0        0       98 2023-05-02 04:17:59.006244 rssfixer-0.1.7/src/rssfixer/__init__.py
+-rw-r--r--   0        0        0    14250 2023-05-29 11:36:19.171314 rssfixer-0.1.7/src/rssfixer/rss.py
+-rw-r--r--   0        0        0    11717 1970-01-01 00:00:00.000000 rssfixer-0.1.7/PKG-INFO
```

### Comparing `rssfixer-0.1.6/LICENSE` & `rssfixer-0.1.7/LICENSE`

 * *Files identical despite different names*

### Comparing `rssfixer-0.1.6/README.md` & `rssfixer-0.1.7/README.md`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # rssfixer
 
 <!-- CODE:BASH:START -->
 <!-- echo '[![GitHub Super-Linter](https://github.com/reuteras/rssfixer/actions/workflows/linter.yml/badge.svg)](https://github.com/marketplace/actions/super-linter)' -->
 <!-- echo '![PyPI](https://img.shields.io/pypi/v/rssfixer?color=green)' -->
 <!-- echo '[![CodeQL](https://github.com/reuteras/rssfixer/workflows/CodeQL/badge.svg)](https://github.com/reuteras/rssfixer/actions?query=workflow%3ACodeQL)' -->
 <!-- echo '[![Coverage](https://raw.githubusercontent.com/reuteras/rssfixer/main/resources/coverage.svg)](https://github.com/reuteras/rssfixer/)' -->
-<!-- if jq '.metrics._totals | ."SEVERITY.HIGH"' resources/bandit.json | grep -vE '^0' > /dev/null ; then echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-red.svg)](https://github.com/PyCQA/bandit)' ; elif jq '.metrics._totals' resources/bandit.json | grep "SEVERITY" | grep -E ' 0,' | wc -l | grep -vE '4$' > /dev/null ; then echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)'; else echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-green.svg)](https://github.com/PyCQA/bandit)' ;fi -->
+<!-- if jq '.metrics._totals | ."SEVERITY.HI"' resources/bandit.json|grep -vE '^0' > /dev/null;then cl='red';elif jq '.metrics._totals' resources/bandit.json|grep "SEVERITY"|grep -E ' 0,'|wc -l|grep -vE '4$' > /dev/null;then cl='yellow';else cl='green';fi echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-' + $cl + '.svg)](https://github.com/PyCQA/bandit)' -->
 <!-- CODE:END -->
 <!-- OUTPUT:START -->
 <!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
 [![GitHub Super-Linter](https://github.com/reuteras/rssfixer/actions/workflows/linter.yml/badge.svg)](https://github.com/marketplace/actions/super-linter)
 ![PyPI](https://img.shields.io/pypi/v/rssfixer?color=green)
 [![CodeQL](https://github.com/reuteras/rssfixer/workflows/CodeQL/badge.svg)](https://github.com/reuteras/rssfixer/actions?query=workflow%3ACodeQL)
 [![Coverage](https://raw.githubusercontent.com/reuteras/rssfixer/main/resources/coverage.svg)](https://github.com/reuteras/rssfixer/)
@@ -70,15 +70,15 @@
 
 An example for [Apple][app]:
 
 ```bash
 rssfixer --title "Apple Security" --output apple.xml --quiet --json --json-entries blogs --json-url slug --base-url https://security.apple.com/blog/ https://security.apple.com/blog
 ```
 
-In this example `--json-entries blogs`specifies that blog entries are located in a key called __blogs__ and that URLs are availablie in a key called __slug__. Since the URL only includes the key (or slug) we specify the full URL to the blog with `--base-url https://security.apple.com/blog/`.
+In this example `--json-entries blogs`specifies that blog entries are located in a key called __blogs__ and that URLs are available in a key called __slug__. Since the URL only includes the key (or slug) we specify the full URL to the blog with `--base-url https://security.apple.com/blog/`.
 
 An example for [truesec.com][tru]:
 
 ```bash
 rssfixer --title Truesec --json --json-description preamble --quiet --output truesec.xml https://www.truesec.com/hub/blog
 ```
 
@@ -117,20 +117,23 @@
 ```Text
 usage: rssfixer [-h] (--html | --json | --list | --release) [--version]
                 [--atom] [--base-url BASE_URL] [--release-url RELEASE_URL]
                 [--release-entries RELEASE_ENTRIES]
                 [--html-entries HTML_ENTRIES] [--html-url HTML_URL]
                 [--html-title HTML_TITLE]
                 [--html-title-class HTML_TITLE_CLASS]
+                [--title-filter TITLE_FILTER]
                 [--html-description HTML_DESCRIPTION]
                 [--html-description-class HTML_DESCRIPTION_CLASS]
                 [--json-entries JSON_ENTRIES] [--json-url JSON_URL]
                 [--json-title JSON_TITLE]
                 [--json-description JSON_DESCRIPTION] [--output OUTPUT]
-                [--title TITLE] [-q] [--stdout]
+                [--title TITLE] [--user-agent USER_AGENT]
+                [--filter-type FILTER_TYPE] [--filter-name FILTER_NAME] [-q]
+                [--stdout]
                 url
 
 Generate RSS feed for blog that don't publish a feed. Default is to find links
 in a simple <ul>-list. Options are available to find links in other HTML
 elements or JSON strings.
 
 positional arguments:
@@ -152,27 +155,35 @@
   --html-entries HTML_ENTRIES
                         HTML selector for entries
   --html-url HTML_URL   HTML selector for URL
   --html-title HTML_TITLE
                         HTML selector for title
   --html-title-class HTML_TITLE_CLASS
                         Flag to specify title class (regex)
+  --title-filter TITLE_FILTER
+                        Filter for title, ignore entries that don't match
   --html-description HTML_DESCRIPTION
                         HTML selector for description
   --html-description-class HTML_DESCRIPTION_CLASS
                         Flag to specify description class (regex)
   --json-entries JSON_ENTRIES
                         JSON key for entries (default: 'entries')
   --json-url JSON_URL   JSON key for URL (default: 'url')
   --json-title JSON_TITLE
                         JSON key for title
   --json-description JSON_DESCRIPTION
                         JSON key for description
   --output OUTPUT       Name of the output file
   --title TITLE         Title of the RSS feed (default: "My RSS Feed")
+  --user-agent USER_AGENT
+                        User agent to use for HTTP requests
+  --filter-type FILTER_TYPE
+                        Filter web page
+  --filter-name FILTER_NAME
+                        Filter web page
   -q, --quiet           Suppress output
   --stdout              Print to stdout
 ```
 
 <!-- OUTPUT:END -->
 
 ## Command-line examples for blogs
@@ -192,15 +203,19 @@
 
 # TrueSec
 # Url: https://www.truesec.com/hub/blog
 rssfixer --title Truesec --output truesec.xml --quiet --json --json-description preamble https://www.truesec.com/hub/blog
 
 # SQLite
 # Url: https://sqlite.org/changes.html
-rssfixer --release --release-entries h3 --release-url https://sqlite.org/download.html https://sqlite.org/changes.html
+rssfixer --title SQLite --release --release-entries h3 --release-url https://sqlite.org/download.html https://sqlite.org/changes.html
+
+# Nucleus
+# https://nucleussec.com/category/cisa-kev
+rssfixer --title "Nucleus CISA KEV" --output nucleus.xml  --html --filter-type div --filter-name recent-post-widget --html-entries div --html-title div --html-title-class "post-desc" --title-filter KEV https://nucleussec.com/category/cisa-kev
 ```
 
 If you have other example use case please add them in [show usage examples][sue] in discussions.
 
 
   [app]: https://security.apple.com/blog/
   [bso]: https://www.crummy.com/software/BeautifulSoup/
```

### Comparing `rssfixer-0.1.6/pyproject.toml` & `rssfixer-0.1.7/pyproject.toml`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 [project]
 name = "rssfixer"
-version = "0.1.5"
 authors = [
   { name="Peter Reuterås", email="peter@reuteras.net" },
 ]
 description = "Generate RSS feed for Wordpress blog without it."
 readme = "README.md"
 requires-python = ">=3.8"
 classifiers = [
@@ -72,41 +71,44 @@
 line-length = 120
 
 [tool.ruff.mccabe]
 # Unlike Flake8, default to a complexity level of 10.
 max-complexity = 10
 [tool.poetry]
 name = "rssfixer"
-version = "0.1.6"
+version = "0.1.7"
 description = "Generate RSS for blogs without a feed."
 authors = ["Peter Reuterås <peter@reuteras.net>"]
 license = "MIT"
 readme = "README.md"
 
 [tool.poetry.dependencies]
 python = "^3.8"
 beautifulsoup4 = "4.12.2"
 feedgen = "0.9.0"
-requests = "2.29.0"
+requests = "2.31.0"
+
+[tool.poetry.group.dev]
+optional = true
 
 [tool.poetry.group.dev.dependencies]
-bandit = "^1.7.5"
-build = "^0.10.0"
-twine = "^4.0.2"
-black = "^23.3.0"
-ruff = "^0.0.263"
-pytest = "^7.3.1"
-requests-mock = "^1.10.0"
-coverage = "^7.2.5"
-pre-commit = "^3.2.2"
+bandit = "*"
+build = "*"
+twine = "*"
+black = "*"
+ruff = "*"
+pytest = "*"
+requests-mock = "*"
+coverage = "*"
+pre-commit = "*"
 
 [tool.poetry.group.github]
 optional = true
 
 [tool.poetry.group.github.dependencies]
-coverage-badge = "^1.1.0"
-markdown-code-runner = "^2.0.0"
+coverage-badge = "*"
+markdown-code-runner = "*"
 
 [tool.black]
 line-length = 120
 target-version = ['py38']
 include = '\.pyi?$'
```

### Comparing `rssfixer-0.1.6/src/rssfixer/rss.py` & `rssfixer-0.1.7/src/rssfixer/rss.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,14 +12,18 @@
 from feedgen.feed import FeedGenerator
 
 try:
     __version__ = "version " + importlib.metadata.version(__package__ or __name__)
 except importlib.metadata.PackageNotFoundError:  # pragma: no cover
     __version__ = "0.0.0"
 
+ua = (
+    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
+)
+
 
 class CheckHtmlAction(argparse.Action):
     """Class to validate argparse for --html options."""
 
     def __call__(self, parser, namespace, values, option_string=None):
         """Validate the class."""
         if not namespace.html:
@@ -43,18 +47,22 @@
     def __call__(self, parser, namespace, values, option_string=None):
         """Validate the class."""
         if not namespace.release:
             parser.error(f"{option_string} requires --release to be specified.")
         setattr(namespace, self.dest, values)
 
 
-def fetch_html(url):
+def fetch_html(arguments):
     """Fetch HTML content from a URL."""
+    url = arguments.url
+    headers = {
+        "User-Agent": arguments.user_agent,
+    }
     try:
-        response = requests.get(url, timeout=10)
+        response = requests.get(url, headers=headers, timeout=10)
     except requests.exceptions.Timeout:  # pragma: no cover
         print("ERROR: Request timed out")
         sys.exit(1)
     except requests.exceptions.ConnectionError:  # pragma: no cover
         print("ERROR: Unable to connect to server")
         sys.exit(1)
     return response.text
@@ -73,14 +81,24 @@
         for item in json_object:
             result = find_entries(item, entries_key)
             if result is not None:
                 return result
     return None
 
 
+def filter_html(soup, filter_type, filter_name):
+    """Filter web page."""
+    filtersoup = soup.find_all(filter_type, filter_name)
+    if not filtersoup:
+        print("ERROR: No entries found")
+        sys.exit(1)
+    filtersoup = BeautifulSoup(str(filtersoup), "html.parser")
+    return filtersoup
+
+
 def extract_links_ul(soup):
     """Extract links from an HTML page with links in <ul>-lists."""
     links = []
     unique_links = set()
 
     # Iterate through all the <ul> elements in the page
     for links_list in soup.find_all("ul", class_=None):
@@ -114,27 +132,32 @@
         try:
             url = entry.findNext(arguments.html_url)["href"]
             title = entry.find(
                 arguments.html_title,
                 re.compile(arguments.html_title_class),
             ).text.strip()
         except (KeyError, AttributeError):
-            print("ERROR: Unable to find URL or title in HTML element")
-            sys.exit(1)
+            # Continue if URL or title is not found to fins other entries
+            continue
         try:
             description = entry.find(
                 arguments.html_description,
                 re.compile(arguments.html_description_class),
             ).text.strip()
         except (KeyError, AttributeError):
             # Ignore description if it's not found
             description = ""
         if url not in unique_links:
-            unique_links.add(url)
-            links.append((url, title, description))
+            if arguments.title_filter:
+                if re.search(arguments.title_filter, title):
+                    unique_links.add(url)
+                    links.append((url, title, description))
+            else:
+                unique_links.add(url)
+                links.append((url, title, description))
 
     if not links:
         print("ERROR: No links found")
         sys.exit(1)
     return links
 
 
@@ -293,14 +316,18 @@
     parser.add_argument(
         "--html-title-class",
         action=CheckHtmlAction,
         default="title",
         help="Flag to specify title class (regex)",
     )
     parser.add_argument(
+        "--title-filter",
+        help="Filter for title, ignore entries that don't match",
+    )
+    parser.add_argument(
         "--html-description",
         action=CheckHtmlAction,
         default="div",
         help="HTML selector for description",
     )
     parser.add_argument(
         "--html-description-class",
@@ -338,14 +365,28 @@
         help="Name of the output file",
     )
     parser.add_argument(
         "--title",
         default="My RSS Feed",
         help='Title of the RSS feed (default: "My RSS Feed")',
     )
+    parser.add_argument(
+        "--user-agent",
+        default=ua,
+        help="User agent to use for HTTP requests",
+    )
+    parser.add_argument(
+        "--filter-type",
+        help="Filter web page",
+    )
+    parser.add_argument(
+        "--filter-name",
+        help="Filter web page",
+    )
+
     parser.add_argument("-q", "--quiet", action="store_true", help="Suppress output")
     parser.add_argument("--stdout", action="store_true", help="Print to stdout")
 
     return parser.parse_args(arguments)
 
 
 def save_rss_feed(rss_feed, arguments):
@@ -372,17 +413,21 @@
     args = parse_arguments(args)
 
     if vars(args).get("version"):
         print(__version__)
         sys.exit(0)
 
     # Get HTML content from URL
-    html_content = fetch_html(args.url)
+    html_content = fetch_html(args)
     soup = BeautifulSoup(html_content, "html.parser")
 
+    # Filter web page
+    if args.filter_type and args.filter_name:
+        soup = filter_html(soup, args.filter_type, args.filter_name)
+
     # Select function to handle different types of blogs
     if args.json:
         links = extract_links_json(soup, args)
     elif args.html:
         links = extract_links_html(soup, args)
     elif args.list:
         links = extract_links_ul(soup)
```

### Comparing `rssfixer-0.1.6/PKG-INFO` & `rssfixer-0.1.7/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 Metadata-Version: 2.1
 Name: rssfixer
-Version: 0.1.6
+Version: 0.1.7
 Summary: Generate RSS for blogs without a feed.
 License: MIT
 Author: Peter Reuterås
 Author-email: peter@reuteras.net
 Requires-Python: >=3.8,<4.0
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Requires-Dist: beautifulsoup4 (==4.12.2)
 Requires-Dist: feedgen (==0.9.0)
-Requires-Dist: requests (==2.29.0)
+Requires-Dist: requests (==2.31.0)
 Description-Content-Type: text/markdown
 
 # rssfixer
 
 <!-- CODE:BASH:START -->
 <!-- echo '[![GitHub Super-Linter](https://github.com/reuteras/rssfixer/actions/workflows/linter.yml/badge.svg)](https://github.com/marketplace/actions/super-linter)' -->
 <!-- echo '![PyPI](https://img.shields.io/pypi/v/rssfixer?color=green)' -->
 <!-- echo '[![CodeQL](https://github.com/reuteras/rssfixer/workflows/CodeQL/badge.svg)](https://github.com/reuteras/rssfixer/actions?query=workflow%3ACodeQL)' -->
 <!-- echo '[![Coverage](https://raw.githubusercontent.com/reuteras/rssfixer/main/resources/coverage.svg)](https://github.com/reuteras/rssfixer/)' -->
-<!-- if jq '.metrics._totals | ."SEVERITY.HIGH"' resources/bandit.json | grep -vE '^0' > /dev/null ; then echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-red.svg)](https://github.com/PyCQA/bandit)' ; elif jq '.metrics._totals' resources/bandit.json | grep "SEVERITY" | grep -E ' 0,' | wc -l | grep -vE '4$' > /dev/null ; then echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)'; else echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-green.svg)](https://github.com/PyCQA/bandit)' ;fi -->
+<!-- if jq '.metrics._totals | ."SEVERITY.HI"' resources/bandit.json|grep -vE '^0' > /dev/null;then cl='red';elif jq '.metrics._totals' resources/bandit.json|grep "SEVERITY"|grep -E ' 0,'|wc -l|grep -vE '4$' > /dev/null;then cl='yellow';else cl='green';fi echo -n '[![security: bandit](https://img.shields.io/badge/security-bandit-' + $cl + '.svg)](https://github.com/PyCQA/bandit)' -->
 <!-- CODE:END -->
 <!-- OUTPUT:START -->
 <!-- ⚠️ This content is auto-generated by `markdown-code-runner`. -->
 [![GitHub Super-Linter](https://github.com/reuteras/rssfixer/actions/workflows/linter.yml/badge.svg)](https://github.com/marketplace/actions/super-linter)
 ![PyPI](https://img.shields.io/pypi/v/rssfixer?color=green)
 [![CodeQL](https://github.com/reuteras/rssfixer/workflows/CodeQL/badge.svg)](https://github.com/reuteras/rssfixer/actions?query=workflow%3ACodeQL)
 [![Coverage](https://raw.githubusercontent.com/reuteras/rssfixer/main/resources/coverage.svg)](https://github.com/reuteras/rssfixer/)
@@ -89,15 +89,15 @@
 
 An example for [Apple][app]:
 
 ```bash
 rssfixer --title "Apple Security" --output apple.xml --quiet --json --json-entries blogs --json-url slug --base-url https://security.apple.com/blog/ https://security.apple.com/blog
 ```
 
-In this example `--json-entries blogs`specifies that blog entries are located in a key called __blogs__ and that URLs are availablie in a key called __slug__. Since the URL only includes the key (or slug) we specify the full URL to the blog with `--base-url https://security.apple.com/blog/`.
+In this example `--json-entries blogs`specifies that blog entries are located in a key called __blogs__ and that URLs are available in a key called __slug__. Since the URL only includes the key (or slug) we specify the full URL to the blog with `--base-url https://security.apple.com/blog/`.
 
 An example for [truesec.com][tru]:
 
 ```bash
 rssfixer --title Truesec --json --json-description preamble --quiet --output truesec.xml https://www.truesec.com/hub/blog
 ```
 
@@ -136,20 +136,23 @@
 ```Text
 usage: rssfixer [-h] (--html | --json | --list | --release) [--version]
                 [--atom] [--base-url BASE_URL] [--release-url RELEASE_URL]
                 [--release-entries RELEASE_ENTRIES]
                 [--html-entries HTML_ENTRIES] [--html-url HTML_URL]
                 [--html-title HTML_TITLE]
                 [--html-title-class HTML_TITLE_CLASS]
+                [--title-filter TITLE_FILTER]
                 [--html-description HTML_DESCRIPTION]
                 [--html-description-class HTML_DESCRIPTION_CLASS]
                 [--json-entries JSON_ENTRIES] [--json-url JSON_URL]
                 [--json-title JSON_TITLE]
                 [--json-description JSON_DESCRIPTION] [--output OUTPUT]
-                [--title TITLE] [-q] [--stdout]
+                [--title TITLE] [--user-agent USER_AGENT]
+                [--filter-type FILTER_TYPE] [--filter-name FILTER_NAME] [-q]
+                [--stdout]
                 url
 
 Generate RSS feed for blog that don't publish a feed. Default is to find links
 in a simple <ul>-list. Options are available to find links in other HTML
 elements or JSON strings.
 
 positional arguments:
@@ -171,27 +174,35 @@
   --html-entries HTML_ENTRIES
                         HTML selector for entries
   --html-url HTML_URL   HTML selector for URL
   --html-title HTML_TITLE
                         HTML selector for title
   --html-title-class HTML_TITLE_CLASS
                         Flag to specify title class (regex)
+  --title-filter TITLE_FILTER
+                        Filter for title, ignore entries that don't match
   --html-description HTML_DESCRIPTION
                         HTML selector for description
   --html-description-class HTML_DESCRIPTION_CLASS
                         Flag to specify description class (regex)
   --json-entries JSON_ENTRIES
                         JSON key for entries (default: 'entries')
   --json-url JSON_URL   JSON key for URL (default: 'url')
   --json-title JSON_TITLE
                         JSON key for title
   --json-description JSON_DESCRIPTION
                         JSON key for description
   --output OUTPUT       Name of the output file
   --title TITLE         Title of the RSS feed (default: "My RSS Feed")
+  --user-agent USER_AGENT
+                        User agent to use for HTTP requests
+  --filter-type FILTER_TYPE
+                        Filter web page
+  --filter-name FILTER_NAME
+                        Filter web page
   -q, --quiet           Suppress output
   --stdout              Print to stdout
 ```
 
 <!-- OUTPUT:END -->
 
 ## Command-line examples for blogs
@@ -211,15 +222,19 @@
 
 # TrueSec
 # Url: https://www.truesec.com/hub/blog
 rssfixer --title Truesec --output truesec.xml --quiet --json --json-description preamble https://www.truesec.com/hub/blog
 
 # SQLite
 # Url: https://sqlite.org/changes.html
-rssfixer --release --release-entries h3 --release-url https://sqlite.org/download.html https://sqlite.org/changes.html
+rssfixer --title SQLite --release --release-entries h3 --release-url https://sqlite.org/download.html https://sqlite.org/changes.html
+
+# Nucleus
+# https://nucleussec.com/category/cisa-kev
+rssfixer --title "Nucleus CISA KEV" --output nucleus.xml  --html --filter-type div --filter-name recent-post-widget --html-entries div --html-title div --html-title-class "post-desc" --title-filter KEV https://nucleussec.com/category/cisa-kev
 ```
 
 If you have other example use case please add them in [show usage examples][sue] in discussions.
 
 
   [app]: https://security.apple.com/blog/
   [bso]: https://www.crummy.com/software/BeautifulSoup/
```

