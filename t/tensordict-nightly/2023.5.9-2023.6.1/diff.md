# Comparing `tmp/tensordict_nightly-2023.5.9-py39-none-any.whl.zip` & `tmp/tensordict_nightly-2023.6.1-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,29 @@
-Zip file size: 130209 bytes, number of entries: 27
--rw-r--r--  2.0 unx     1088 b- defN 23-May-09 11:18 tensordict/__init__.py
--rw-r--r--  2.0 unx     6000 b- defN 23-May-09 11:18 tensordict/_contextlib.py
--rw-r--r--  2.0 unx    29601 b- defN 23-May-09 11:18 tensordict/memmap.py
--rw-r--r--  2.0 unx    33049 b- defN 23-May-09 11:18 tensordict/persistent.py
--rw-r--r--  2.0 unx    30420 b- defN 23-May-09 11:18 tensordict/tensorclass.py
--rw-r--r--  2.0 unx   267149 b- defN 23-May-09 11:18 tensordict/tensordict.py
--rw-r--r--  2.0 unx    28030 b- defN 23-May-09 11:18 tensordict/utils.py
--rw-r--r--  2.0 unx       84 b- defN 23-May-09 11:19 tensordict/version.py
--rw-r--r--  2.0 unx     1314 b- defN 23-May-09 11:18 tensordict/nn/__init__.py
--rw-r--r--  2.0 unx    35649 b- defN 23-May-09 11:18 tensordict/nn/common.py
--rw-r--r--  2.0 unx    19188 b- defN 23-May-09 11:18 tensordict/nn/functional_modules.py
--rw-r--r--  2.0 unx    22301 b- defN 23-May-09 11:18 tensordict/nn/probabilistic.py
--rw-r--r--  2.0 unx    19564 b- defN 23-May-09 11:18 tensordict/nn/sequence.py
--rw-r--r--  2.0 unx    10621 b- defN 23-May-09 11:18 tensordict/nn/utils.py
--rw-r--r--  2.0 unx      499 b- defN 23-May-09 11:18 tensordict/nn/distributions/__init__.py
--rw-r--r--  2.0 unx     7073 b- defN 23-May-09 11:18 tensordict/nn/distributions/continuous.py
--rw-r--r--  2.0 unx     2580 b- defN 23-May-09 11:18 tensordict/nn/distributions/discrete.py
--rw-r--r--  2.0 unx     6504 b- defN 23-May-09 11:18 tensordict/nn/distributions/truncated_normal.py
--rw-r--r--  2.0 unx     1226 b- defN 23-May-09 11:18 tensordict/nn/distributions/utils.py
--rw-r--r--  2.0 unx      381 b- defN 23-May-09 11:18 tensordict/prototype/__init__.py
--rw-r--r--  2.0 unx     7507 b- defN 23-May-09 11:18 tensordict/prototype/fx.py
--rw-r--r--  2.0 unx      739 b- defN 23-May-09 11:18 tensordict/prototype/tensorclass.py
--rw-r--r--  2.0 unx     1098 b- defN 23-May-09 11:19 tensordict_nightly-2023.5.9.dist-info/LICENSE
--rw-r--r--  2.0 unx    15208 b- defN 23-May-09 11:19 tensordict_nightly-2023.5.9.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-May-09 11:19 tensordict_nightly-2023.5.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       11 b- defN 23-May-09 11:19 tensordict_nightly-2023.5.9.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2352 b- defN 23-May-09 11:19 tensordict_nightly-2023.5.9.dist-info/RECORD
-27 files, 549329 bytes uncompressed, 126407 bytes compressed:  77.0%
+Zip file size: 132631 bytes, number of entries: 27
+-rw-r--r--  2.0 unx     1088 b- defN 23-Jun-01 11:18 tensordict/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 23-Jun-01 11:18 tensordict/_contextlib.py
+-rw-r--r--  2.0 unx    30428 b- defN 23-Jun-01 11:18 tensordict/memmap.py
+-rw-r--r--  2.0 unx    33747 b- defN 23-Jun-01 11:18 tensordict/persistent.py
+-rw-r--r--  2.0 unx    30420 b- defN 23-Jun-01 11:18 tensordict/tensorclass.py
+-rw-r--r--  2.0 unx   274343 b- defN 23-Jun-01 11:18 tensordict/tensordict.py
+-rw-r--r--  2.0 unx    28030 b- defN 23-Jun-01 11:18 tensordict/utils.py
+-rw-r--r--  2.0 unx       84 b- defN 23-Jun-01 11:19 tensordict/version.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-Jun-01 11:18 tensordict/nn/__init__.py
+-rw-r--r--  2.0 unx    37534 b- defN 23-Jun-01 11:18 tensordict/nn/common.py
+-rw-r--r--  2.0 unx    19059 b- defN 23-Jun-01 11:18 tensordict/nn/functional_modules.py
+-rw-r--r--  2.0 unx    22301 b- defN 23-Jun-01 11:18 tensordict/nn/probabilistic.py
+-rw-r--r--  2.0 unx    19564 b- defN 23-Jun-01 11:18 tensordict/nn/sequence.py
+-rw-r--r--  2.0 unx    10621 b- defN 23-Jun-01 11:18 tensordict/nn/utils.py
+-rw-r--r--  2.0 unx      499 b- defN 23-Jun-01 11:18 tensordict/nn/distributions/__init__.py
+-rw-r--r--  2.0 unx     7073 b- defN 23-Jun-01 11:18 tensordict/nn/distributions/continuous.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Jun-01 11:18 tensordict/nn/distributions/discrete.py
+-rw-r--r--  2.0 unx     6504 b- defN 23-Jun-01 11:18 tensordict/nn/distributions/truncated_normal.py
+-rw-r--r--  2.0 unx     1226 b- defN 23-Jun-01 11:18 tensordict/nn/distributions/utils.py
+-rw-r--r--  2.0 unx      381 b- defN 23-Jun-01 11:18 tensordict/prototype/__init__.py
+-rw-r--r--  2.0 unx     7507 b- defN 23-Jun-01 11:18 tensordict/prototype/fx.py
+-rw-r--r--  2.0 unx      739 b- defN 23-Jun-01 11:18 tensordict/prototype/tensorclass.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-Jun-01 11:19 tensordict_nightly-2023.6.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    15208 b- defN 23-Jun-01 11:19 tensordict_nightly-2023.6.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-Jun-01 11:19 tensordict_nightly-2023.6.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 23-Jun-01 11:19 tensordict_nightly-2023.6.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2352 b- defN 23-Jun-01 11:19 tensordict_nightly-2023.6.1.dist-info/RECORD
+27 files, 559804 bytes uncompressed, 128829 bytes compressed:  77.0%
```

## zipnote {}

```diff
@@ -60,23 +60,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2023.5.9.dist-info/LICENSE
+Filename: tensordict_nightly-2023.6.1.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2023.5.9.dist-info/METADATA
+Filename: tensordict_nightly-2023.6.1.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2023.5.9.dist-info/WHEEL
+Filename: tensordict_nightly-2023.6.1.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2023.5.9.dist-info/top_level.txt
+Filename: tensordict_nightly-2023.6.1.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2023.5.9.dist-info/RECORD
+Filename: tensordict_nightly-2023.6.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/memmap.py

```diff
@@ -358,22 +358,35 @@
             dtype=TORCH_TO_NUMPY_DTYPE_DICT[self.dtype],
             mode="r",
             shape=self.np_shape,
         )
 
     def _get_item(self, idx: IndexType, memmap_array: np.ndarray) -> np.ndarray:
         if isinstance(idx, torch.Tensor):
-            idx = idx.cpu()
+            # indexing a numpy.memmap with a torch.Tensor doesn't behave as expected, we
+            # convert to numpy.ndarray for behaviour that is consistent with indexing
+            # a torch.Tensor with a torch.Tensor
+            idx = idx.cpu().numpy()
         elif isinstance(idx, tuple) and any(
             isinstance(sub_index, torch.Tensor) for sub_index in idx
         ):
             idx = tuple(
-                sub_index.cpu() if isinstance(sub_index, torch.Tensor) else sub_index
+                # see above comment about indexing numpy.memmap with torch.Tensor
+                sub_index.cpu().numpy()
+                if isinstance(sub_index, torch.Tensor)
+                else sub_index
                 for sub_index in idx
             )
+        elif isinstance(idx, list):
+            # wrapping list index in tuple to avoid following warning when indexing
+            # FutureWarning: Using a non-tuple sequence for multidimensional indexing
+            # is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future
+            # this will be interpreted as an array index, `arr[np.array(seq)]`, which
+            # will result either in an error or a different result.
+            idx = (idx,)
         memmap_array = memmap_array[idx]
         return memmap_array
 
     def _load_item(
         self,
         idx: int | tuple | list | None = None,
         memmap_array: np.ndarray | None = None,
```

## tensordict/persistent.py

```diff
@@ -223,15 +223,15 @@
     def _process_key(self, key):
         if isinstance(key, str):
             return key
         else:
             return "/".join(key)
 
     def _check_batch_size(self, batch_size) -> None:
-        for key in self.keys(True, True):
+        for key in self.keys(include_nested=True, leaves_only=True):
             key = self._process_key(key)
             size = self.file[key].shape
             if torch.Size(size[: len(batch_size)]) != batch_size:
                 raise ValueError(
                     f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
                 )
 
@@ -342,15 +342,15 @@
             return tuple(cls._process_index(_idx, array) for _idx in idx)
         if isinstance(idx, torch.Tensor):
             # if idx.dtype == torch.bool:
             #     # expand to the right
             #     print(idx.shape, array.shape, idx.sum())
             #     idx = expand_right(idx, array.shape)
             return idx.cpu().detach().numpy()
-        if isinstance(idx, list):
+        if isinstance(idx, (range, list)):
             return np.asarray(idx)
         return idx
 
     def __getitem__(self, item):
         if isinstance(item, str) or (
             isinstance(item, tuple) and all(isinstance(val, str) for val in item)
         ):
@@ -468,15 +468,15 @@
     def is_contiguous(self):
         return False
 
     def masked_fill(self, mask, value):
         return self.to_tensordict().masked_fill(mask, value)
 
     def masked_fill_(self, mask, value):
-        for key in self.keys(True, True):
+        for key in self.keys(include_nested=True, leaves_only=True):
             array = self._get_array(key)
             array[expand_right(mask, array.shape).cpu().numpy()] = value
         return self
 
     def memmap_(
         self, prefix: str | None = None, copy_existing: bool = False
     ) -> PersistentTensorDict:
@@ -486,15 +486,15 @@
 
     def memmap(
         self,
         prefix: str | None = None,
     ) -> TensorDict:
         """Converts the PersistentTensorDict to a memmap equivalent."""
         mm_like = self.memmap_like(prefix)
-        for key in self.keys(True, True):
+        for key in self.keys(include_nested=True, leaves_only=True):
             mm_val = mm_like[key]
             mm_val._memmap_array[:] = self._get_array(key)
         return mm_like
 
     def memmap_like(self, prefix: str | None = None) -> TensorDictBase:
         # re-implements this to make it faster using the meta-data
         if prefix is not None:
@@ -795,39 +795,52 @@
         key = self._process_key(key)
         if key not in visitor:
             raise KeyError(f'key "{key}" not found in h5.')
         # we don't need to check shape as the modification will be done
         # in-place and an error will be thrown anyway if shapes don't match
         return self._set(key, value, inplace=True, idx=idx, check_shape=False)
 
+    def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
+        for key, td in orig_metadata_container._nested_tensordicts.items():
+            array = self._get_array(key)
+            self._nested_tensordicts[key] = PersistentTensorDict(
+                group=array,
+                batch_size=td.batch_size,
+                device=td.device,
+            )
+            self._nested_tensordicts[key].names = td._names
+            self._nested_tensordicts[key]._set_metadata(td)
+
     def clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
         if recurse:
             # this should clone the h5 to a new location indicated by newfile
             if newfile is None:
                 warnings.warn(
                     "A destination should be provided when cloning a "
                     "PersistentTensorDict. A temporary file will be used "
                     "instead. Use `recurse=False` to keep track of the original data "
                     "with a new PersistentTensorDict instance."
                 )
                 tmpfile = tempfile.NamedTemporaryFile()
                 newfile = tmpfile.name
             f_dest = h5py.File(newfile, "w")
             f_src = self.file
-            for key in self.keys(True, True):
+            for key in self.keys(include_nested=True, leaves_only=True):
                 key = self._process_key(key)
                 f_dest.create_dataset(key, data=f_src[key], **self.kwargs)
                 # f_src.copy(f_src[key],  f_dest[key], "DataSet")
             # create a non-recursive copy and update the file
             # this way, we can keep the batch-size of every nested tensordict
             clone = self.clone(False)
-            clone.file = f_src
+            clone.file = f_dest
             clone.filename = newfile
             clone._pin_mem = False
             clone.names = self._names
+            clone._nested_tensordicts = {}
+            clone._set_metadata(self)
             return clone
         else:
             # we need to keep the batch-size of nested tds, which we do manually
             nested_tds = {
                 key: td.clone(False) for key, td in self._nested_tensordicts.items()
             }
             filename = self.filename
```

## tensordict/tensordict.py

```diff
@@ -5,14 +5,15 @@
 
 from __future__ import annotations
 
 import abc
 import collections
 import functools
 import numbers
+import re
 import textwrap
 import warnings
 from collections import defaultdict
 from collections.abc import MutableMapping
 from copy import copy, deepcopy
 from numbers import Number
 from pathlib import Path
@@ -1221,14 +1222,18 @@
             key (str, tuple of str): key to be queried. If tuple of str it is
                 equivalent to chained calls of getattr.
             default: default value if the key is not found in the tensordict.
 
         """
         raise NotImplementedError(f"{self.__class__.__name__}")
 
+    def get_item_shape(self, key: NestedKey):
+        """Returns the shape of the entry."""
+        return self.get(key).shape
+
     def pop(
         self, key: NestedKey, default: str | CompatibleType = NO_DEFAULT
     ) -> CompatibleType:
         _nested_key_check(key)
         try:
             # using try/except for get/del is suboptimal, but
             # this is faster that checkink if key in self keys
@@ -1257,14 +1262,15 @@
         return self.apply(fn, inplace=True)
 
     def apply(
         self,
         fn: Callable,
         *others: TensorDictBase,
         batch_size: Sequence[int] | None = None,
+        device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         **constructor_kwargs,
     ) -> TensorDictBase:
         """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
 
         The apply method will return an TensorDict instance, regardless of the
@@ -1280,14 +1286,15 @@
                 current tensordict. The :obj:`fn` argument should receive as many
                 inputs as the number of tensordicts, including the one where apply is
                 being called.
             batch_size (sequence of int, optional): if provided,
                 the resulting TensorDict will have the desired batch_size.
                 The :obj:`batch_size` argument should match the batch_size after
                 the transformation. This is a keyword only argument.
+            device (torch.device, optional): the resulting device, if any.
             names (list of str, optional): the new dimension names, in case the
                 batch_size is modified.
             inplace (bool, optional): if True, changes are made in-place.
                 Default is False. This is a keyword only argument.
             **constructor_kwargs: additional keyword arguments to be passed to the
                 TensorDict constructor.
 
@@ -1306,23 +1313,23 @@
         if inplace:
             out = self
         elif batch_size is not None:
             out = TensorDict(
                 {},
                 batch_size=torch.Size(batch_size),
                 names=names,
-                device=self.device,
+                device=self.device if not device else device,
                 _run_checks=False,
                 **constructor_kwargs,
             )
         else:
             out = TensorDict(
                 {},
                 batch_size=self.batch_size,
-                device=self.device,
+                device=self.device if not device else device,
                 names=self._names,
                 _run_checks=False,
                 **constructor_kwargs,
             )
 
         is_locked = out.is_locked
         if not inplace and is_locked:
@@ -1332,14 +1339,15 @@
             _others = [_other.get(key) for _other in others]
             if is_tensor_collection(item):
                 item_trsf = item.apply(
                     fn,
                     *_others,
                     inplace=inplace,
                     batch_size=batch_size,
+                    device=device,
                     **constructor_kwargs,
                 )
             else:
                 item_trsf = fn(item, *_others)
             if item_trsf is not None:
                 # if `self` is a `SubTensorDict` we want to process the input,
                 # hence we call `set` rather than `_set`.
@@ -1538,28 +1546,29 @@
                 value = self._convert_to_tensor(value)
             except ValueError:
                 raise ValueError(
                     f"we only supports tensorclasses, tensordicts,"
                     f" numeric scalars and tensors. Got {type(value)}"
                 )
 
-        if self.device is not None:
-            value = value.to(self.device)
-
         if check_shape and _shape(value)[: self.batch_dims] != self.batch_size:
             # if TensorDict, let's try to map it to the desired shape
             if is_tensor_collection(value):
                 value = value.clone(recurse=False)
                 value.batch_size = self.batch_size
             else:
                 raise RuntimeError(
                     f"batch dimension mismatch, got self.batch_size"
                     f"={self.batch_size} and value.shape[:self.batch_dims]"
                     f"={_shape(value)[: self.batch_dims]} with value {value}"
                 )
+
+        if self.device is not None:
+            value = value.to(self.device, non_blocking=True)
+
         if (
             self._names is not None
             and is_tensor_collection(value)
             and check_shape
             and value.names[: self.ndim] != self.names
         ):
             value = value.clone(False).refine_names(*self.names)
@@ -1886,18 +1895,16 @@
         target = self if inplace else self.clone(recurse=False)
         # is_nested = any((type(key) is tuple) for key in keys)
         # if len(keys) > 1:
         #     tdkeys = set(self.keys(is_nested))
         # else:
         #     tdkeys = self.keys(is_nested)
         for key in keys:
-            try:
+            if key in self.keys(True):
                 del target[key]
-            except KeyError:
-                continue
         return target
 
     @abc.abstractmethod
     def set_at_(
         self, key: NestedKey, value: CompatibleType, idx: IndexType
     ) -> TensorDictBase:
         """Sets the values in-place at the index indicated by :obj:`idx`.
@@ -2220,30 +2227,31 @@
             _idx_start = _idx_end
             _idx_end = _idx_end + interval if c < chunks - 2 else self.batch_size[dim]
         if dim < 0:
             dim = len(self.batch_size) + dim
         return tuple(self[(*[slice(None) for _ in range(dim)], idx)] for idx in indices)
 
     def clone(self, recurse: bool = True) -> TensorDictBase:
-        """Clones a TensorDictBase subclass instance onto a new TensorDict.
+        """Clones a TensorDictBase subclass instance onto a new TensorDictBase subclass of the same type.
+
+        To create a TensorDict instance from any other TensorDictBase subtype, call the :meth:`~.to_tensordict` method
+        instead.
 
         Args:
             recurse (bool, optional): if True, each tensor contained in the
                 TensorDict will be copied too. Default is `True`.
 
+        .. note::
+          For some TensorDictBase subtypes, such as :class:`SubTensorDict`, cloning
+          recursively makes little sense (in this specific case it would involve
+          copying the parent tensordict too). In those cases, :meth:`~.clone` will
+          fall back onto :meth:`~.to_tensordict`.
+
         """
-        return TensorDict(
-            source={key: _clone_value(value, recurse) for key, value in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            names=copy(self._names),
-            _run_checks=False,
-            _is_shared=self.is_shared() if not recurse else False,
-            _is_memmap=self.is_memmap() if not recurse else False,
-        )
+        raise NotImplementedError
 
     @classmethod
     def __torch_function__(
         cls,
         func: Callable,
         types: tuple[type, ...],
         args: tuple[Any, ...] = (),
@@ -2290,16 +2298,18 @@
     def _change_batch_size(self, new_size: torch.Size) -> None:
         raise NotImplementedError
 
     def cpu(self) -> TensorDictBase:
         """Casts a tensordict to CPU."""
         return self.to("cpu")
 
-    def cuda(self, device: int = 0) -> TensorDictBase:
+    def cuda(self, device: int = None) -> TensorDictBase:
         """Casts a tensordict to a cuda device (if not already on it)."""
+        if device is None:
+            return self.to(torch.device("cuda"))
         return self.to(f"cuda:{device}")
 
     @abc.abstractmethod
     def masked_fill_(self, mask: Tensor, value: float | bool) -> TensorDictBase:
         """Fills the values corresponding to the mask with the desired value.
 
         Args:
@@ -2359,21 +2369,22 @@
             >>> mask = torch.tensor([True, False, False])
             >>> td_mask = td.masked_select(mask)
             >>> td_mask.get("a")
             tensor([[0., 0., 0., 0.]])
 
         """
         d = {}
+        mask_expand = mask
+        while mask_expand.ndimension() > self.batch_dims:
+            mndim = mask_expand.ndimension()
+            mask_expand = mask_expand.squeeze(-1)
+            if mndim == mask_expand.ndimension():  # no more squeeze
+                break
         for key, value in self.items():
-            while mask.ndimension() > self.batch_dims:
-                mask_expand = mask.squeeze(-1)
-            else:
-                mask_expand = mask
-            value_select = value[mask_expand]
-            d[key] = value_select
+            d[key] = value[mask_expand]
         dim = int(mask.sum().item())
         other_dim = self.shape[mask.ndim :]
         return TensorDict(
             device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
         )
 
     @abc.abstractmethod
@@ -3241,14 +3252,54 @@
         for key in self.keys():
             if is_tensor_collection(self.entry_class(key)):
                 self.get(key).unlock_()
         return self
 
     unlock = _renamed_inplace_method(unlock_)
 
+    def is_floating_point(self):
+        for item in self.values(include_nested=True, leaves_only=True):
+            if not item.is_floating_point():
+                return False
+        else:
+            return True
+
+    def double(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self.apply(lambda x: x.double())
+
+    def float(self):
+        r"""Casts all tensors to ``torch.float``."""
+        return self.apply(lambda x: x.float())
+
+    def int(self):
+        r"""Casts all tensors to ``torch.int``."""
+        return self.apply(lambda x: x.int())
+
+    def bool(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self.apply(lambda x: x.bool())
+
+    def half(self):
+        r"""Casts all tensors to ``torch.half``."""
+        return self.apply(lambda x: x.half())
+
+    def bfloat16(self):
+        r"""Casts all tensors to ``torch.bfloat16``."""
+        return self.apply(lambda x: x.bfloat16())
+
+    def type(self, dst_type):
+        r"""Casts all tensors to :attr:`dst_type`.
+
+        Args:
+            dst_type (type or string): the desired type
+
+        """
+        return self.apply(lambda x: x.type(dst_type))
+
 
 class TensorDict(TensorDictBase):
     """A batched dictionary of tensors.
 
     TensorDict is a tensor container where all tensors are stored in a
     key-value pair fashion and where each element shares at least the
     following features:
@@ -3987,21 +4038,18 @@
             return td
         elif isinstance(dest, (torch.device, str, int)):
             # must be device
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
 
-            self_copy = TensorDict(
-                {key: value.to(dest, **kwargs) for key, value in self.items()},
-                batch_size=self.batch_size,
-                device=dest,
-                names=self._names,
-            )
-            return self_copy
+            def to(tensor):
+                return tensor.to(dest, **kwargs)
+
+            return self.apply(to, device=dest)
         elif isinstance(dest, torch.Size):
             self.batch_size = dest
             return self
         else:
             raise NotImplementedError(
                 f"dest must be a string, torch.device or a TensorDict "
                 f"instance, {dest} not allowed"
@@ -4016,14 +4064,25 @@
     def masked_fill(self, mask: Tensor, value: float | bool) -> TensorDictBase:
         td_copy = self.clone()
         return td_copy.masked_fill_(mask, value)
 
     def is_contiguous(self) -> bool:
         return all([value.is_contiguous() for _, value in self.items()])
 
+    def clone(self, recurse: bool = True) -> TensorDictBase:
+        return TensorDict(
+            source={key: _clone_value(value, recurse) for key, value in self.items()},
+            batch_size=self.batch_size,
+            device=self.device,
+            names=copy(self._names),
+            _run_checks=False,
+            _is_shared=self.is_shared() if not recurse else False,
+            _is_memmap=self.is_memmap() if not recurse else False,
+        )
+
     def contiguous(self) -> TensorDictBase:
         if not self.is_contiguous():
             return self.clone()
         return self
 
     def select(
         self, *keys: NestedKey, inplace: bool = False, strict: bool = True
@@ -4323,14 +4382,28 @@
         raise RuntimeError(
             f"keyword arguments {list(kwargs.keys())} are not "
             f"supported with full_like with TensorDict"
         )
     return td_clone
 
 
+@implements_for_td(torch.empty_like)
+def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
+    try:
+        tdclone = td.clone()
+    except Exception as err:
+        raise RuntimeError(
+            "The tensordict passed to torch.empty_like cannot be "
+            "cloned, preventing empty_like to be called. "
+            "Consider calling tensordict.to_tensordict() first."
+        ) from err
+
+    return tdclone.apply_(lambda x: torch.empty_like(x, *args, **kwargs))
+
+
 @implements_for_td(torch.clone)
 def _clone(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
     return td.clone(*args, **kwargs)
 
 
 @implements_for_td(torch.squeeze)
 def _squeeze(td: TensorDictBase, *args: Any, **kwargs: Any) -> TensorDictBase:
@@ -5133,17 +5206,59 @@
         return self._source
 
     def del_(self, key: str) -> TensorDictBase:
         self._source = self._source.del_(key)
         return self
 
     def clone(self, recurse: bool = True) -> SubTensorDict:
+        """Clones the SubTensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`TensorDict` instance will be created from the :class:`SubTensorDict`.
+                Otherwise, another :class:`SubTensorDict` with identical content
+                will be returned.
+
+        Examples:
+            >>> data = TensorDict({"a": torch.arange(4).reshape(2, 2,)}, batch_size=[2, 2])
+            >>> sub_data = data.get_sub_tensordict([0,])
+            >>> print(sub_data)
+            SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> # the data of both subtensordict is the same
+            >>> print(data.get("a").data_ptr(), sub_data.get("a").data_ptr())
+            140183705558208 140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=True)
+            >>> print(sub_data_clone)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>. print(sub_data.get("a").data_ptr())
+            140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=False)
+            >>> print(sub_data_clone)
+            SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> print(sub_data.get("a").data_ptr())
+            140183705558208
+        """
         if not recurse:
-            return copy(self)
-        return SubTensorDict(source=self._source, idx=self.idx)
+            return SubTensorDict(source=self._source.clone(recurse=False), idx=self.idx)
+        return self.to_tensordict()
 
     def is_contiguous(self) -> bool:
         return all(value.is_contiguous() for value in self.values())
 
     def contiguous(self) -> TensorDictBase:
         if self.is_contiguous():
             return self
@@ -5372,14 +5487,44 @@
     def _rename_subtds(self, names):
         # remove the name of the stack dim
         names = list(names)
         del names[self.stack_dim]
         for td in self.tensordicts:
             td.names = names
 
+    def get_item_shape(self, key):
+        """Gets the shape of an item in the lazy stack.
+
+        Heterogeneous dimensions are returned as -1.
+
+        This implementation is inefficient as it will attempt to stack the items
+        to compute their shape, and should only be used for printing.
+        """
+        try:
+            item = self.get(key)
+            return item.shape
+        except RuntimeError as err:
+            if re.match(r"Found more than one unique shape in the tensors", str(err)):
+                shape = None
+                for td in self.tensordicts:
+                    if shape is None:
+                        shape = list(td.get_item_shape(key))
+                    else:
+                        _shape = td.get_item_shape(key)
+                        if len(shape) != len(_shape):
+                            shape = [-1]
+                            return torch.Size(shape)
+                        shape = [
+                            s1 if s1 == s2 else -1 for (s1, s2) in zip(shape, _shape)
+                        ]
+                shape.insert(self.stack_dim, len(self.tensordicts))
+                return torch.Size(shape)
+            else:
+                raise err
+
     def is_shared(self) -> bool:
         are_shared = [td.is_shared() for td in self.tensordicts]
         are_shared = [value for value in are_shared if value is not None]
         if not len(are_shared):
             return None
         if any(are_shared) and not all(are_shared):
             raise RuntimeError(
@@ -6617,23 +6762,32 @@
             device=self.device,
             _run_checks=False,
             _is_memmap=self.is_memmap(),
             _is_shared=self.is_shared(),
         ).exclude(*keys, inplace=True)
 
     def clone(self, recurse: bool = True) -> TensorDictBase:
+        """Clones the Lazy TensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`TensorDict` instance will be returned.
+                Otherwise, another :class:`SubTensorDict` with identical content
+                will be returned.
+        """
         if not recurse:
-            return copy(self)
-        return TensorDict(
-            source=self.to_dict(),
-            batch_size=self.batch_size,
-            device=self.device,
-            names=self._names,
-            _run_checks=False,
-        )
+            return type(self)(
+                source=self._source.clone(False),
+                custom_op=self.custom_op,
+                inv_op=self.inv_op,
+                custom_op_kwargs=self.custom_op_kwargs,
+                inv_op_kwargs=self.inv_op_kwargs,
+                batch_size=self.batch_size,
+            )
+        return self.to_tensordict()
 
     def is_contiguous(self) -> bool:
         return all([value.is_contiguous() for _, value in self.items()])
 
     def contiguous(self) -> TensorDictBase:
         if self.is_contiguous():
             return self
@@ -6996,24 +7150,57 @@
             f"dtype={_dtype(tensor)}",
             f"is_shared={_is_shared(tensor)}",
         ]
     )
     return f"{tensor.__class__.__name__}({s})"
 
 
+def _get_repr_custom(cls, shape, device, dtype, is_shared) -> str:
+    s = ", ".join(
+        [
+            f"shape={shape}",
+            f"device={device}",
+            f"dtype={dtype}",
+            f"is_shared={is_shared}",
+        ]
+    )
+    return f"{cls.__name__}({s})"
+
+
 def _make_repr(key: str, item: CompatibleType, tensordict: TensorDictBase) -> str:
     if is_tensor_collection(type(item)):
         return f"{key}: {repr(tensordict.get(key))}"
     return f"{key}: {_get_repr(item)}"
 
 
 def _td_fields(td: TensorDictBase) -> str:
+    strs = []
+    for key in td.keys():
+        try:
+            item = td.get(key)
+            strs.append(_make_repr(key, item, td))
+        except RuntimeError as err:
+            if re.match(r"Found more than one unique shape in the tensors", str(err)):
+                # we know td is lazy stacked and the key is a leaf
+                # so we can get the shape and escape the error
+                shape = td.get_item_shape(key)
+                tensor = td.tensordicts[0].get(key)
+                substr = _get_repr_custom(
+                    tensor.__class__,
+                    shape=shape,
+                    device=tensor.device,
+                    dtype=tensor.dtype,
+                    is_shared=tensor.is_shared(),
+                )
+                strs.append(f"{key}: {substr}")
+            else:
+                raise err
+
     return indent(
-        "\n"
-        + ",\n".join(sorted([_make_repr(key, item, td) for key, item in td.items()])),
+        "\n" + ",\n".join(sorted(strs)),
         4 * " ",
     )
 
 
 def _check_keys(
     list_of_tensordicts: Sequence[TensorDictBase],
     strict: bool = False,
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.05.09'
-git_version = '961bf9d4da45cb3720409609bd3099de7cd3ae71'
+__version__ = '2023.06.01'
+git_version = '6f7b3df7243c95dc224d460a5d0295753475b951'
```

## tensordict/nn/common.py

```diff
@@ -8,14 +8,16 @@
 import functools
 import inspect
 import warnings
 from textwrap import indent
 from typing import Any, Callable, Iterable, List, Sequence, Tuple, Union
 
 import torch
+from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
+
 from tensordict.nn.functional_modules import make_functional
 
 from tensordict.nn.utils import set_skip_existing
 from tensordict.tensordict import is_tensor_collection, make_tensordict, TensorDictBase
 from tensordict.utils import _normalize_key, _seq_of_nested_key_check, NestedKey
 from torch import nn, Tensor
 
@@ -513,14 +515,24 @@
                     x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                     z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
                 batch_size=torch.Size([]),
                 device=None,
                 is_shared=False)
 
         """
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
         self.register_forward_hook(_OutKeysSelect(out_keys))
         for hook in self._forward_hooks.values():
             hook._init(self)
         return self
 
     def reset_out_keys(self):
         """Resets the ``out_keys`` attribute to its orignal value.
@@ -772,52 +784,74 @@
         self,
         tensordict: TensorDictBase,
         *args,
         tensordict_out: TensorDictBase | None = None,
         **kwargs: Any,
     ) -> TensorDictBase:
         """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
-        if len(args):
-            tensordict_out = args[0]
-            args = args[1:]
-            # we will get rid of tensordict_out as a regular arg, because it
-            # blocks us when using vmap
-            # with stateful but functional modules: the functional module checks if
-            # it still contains parameters. If so it considers that only a "params" kwarg
-            # is indicative of what the params are, when we could potentially make a
-            # special rule for TensorDictModule that states that the second arg is
-            # likely to be the module params.
-            warnings.warn(
-                "tensordict_out will be deprecated soon.", category=DeprecationWarning
-            )
-        if len(args):
-            raise ValueError(
-                "Got a non-empty list of extra agruments, when none was expected."
-            )
-        tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
         try:
-            tensors = self._call_module(tensors, **kwargs)
+            if len(args):
+                tensordict_out = args[0]
+                args = args[1:]
+                # we will get rid of tensordict_out as a regular arg, because it
+                # blocks us when using vmap
+                # with stateful but functional modules: the functional module checks if
+                # it still contains parameters. If so it considers that only a "params" kwarg
+                # is indicative of what the params are, when we could potentially make a
+                # special rule for TensorDictModule that states that the second arg is
+                # likely to be the module params.
+                warnings.warn(
+                    "tensordict_out will be deprecated soon.",
+                    category=DeprecationWarning,
+                )
+            if len(args):
+                raise ValueError(
+                    "Got a non-empty list of extra agruments, when none was expected."
+                )
+            tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
+            try:
+                tensors = self._call_module(tensors, **kwargs)
+            except Exception as err:
+                if any(tensor is None for tensor in tensors) and "None" in str(err):
+                    none_set = {
+                        key
+                        for key, tensor in zip(self.in_keys, tensors)
+                        if tensor is None
+                    }
+                    raise KeyError(
+                        "Some tensors that are necessary for the module call may "
+                        "not have not been found in the input tensordict: "
+                        f"the following inputs are None: {none_set}."
+                    ) from err
+                else:
+                    raise err
+            if isinstance(tensors, (dict, TensorDictBase)):
+                tensors = tuple(tensors.get(key, None) for key in self.out_keys)
+            if not isinstance(tensors, tuple):
+                tensors = (tensors,)
+            tensordict_out = self._write_to_tensordict(
+                tensordict, tensors, tensordict_out
+            )
+            return tensordict_out
         except Exception as err:
-            if any(tensor is None for tensor in tensors) and "None" in str(err):
-                none_set = {
-                    key for key, tensor in zip(self.in_keys, tensors) if tensor is None
-                }
-                raise KeyError(
-                    "Some tensors that are necessary for the module call may "
-                    "not have not been found in the input tensordict: "
-                    f"the following inputs are None: {none_set}."
-                ) from err
-            else:
-                raise err
-        if isinstance(tensors, (dict, TensorDictBase)):
-            tensors = tuple(tensors.get(key, None) for key in self.out_keys)
-        if not isinstance(tensors, tuple):
-            tensors = (tensors,)
-        tensordict_out = self._write_to_tensordict(tensordict, tensors, tensordict_out)
-        return tensordict_out
+            module = self.module
+            if not isinstance(module, nn.Module):
+                try:
+                    import inspect
+
+                    module = inspect.getsource(module)
+                except OSError:
+                    # then we can't print the source code
+                    pass
+            module = indent(str(module), 4 * " ")
+            in_keys = indent(f"in_keys={self.in_keys}", 4 * " ")
+            out_keys = indent(f"out_keys={self.out_keys}", 4 * " ")
+            raise RuntimeError(
+                f"TensorDictModule failed with operation\n{module}\n{in_keys}\n{out_keys}."
+            ) from err
 
     @property
     def device(self) -> torch.device:
         for p in self.parameters():
             return p.device
         return torch.device("cpu")
 
@@ -837,14 +871,25 @@
             return super().__getattr__(name)
         except AttributeError as err1:
             try:
                 return getattr(super().__getattr__("module"), name)
             except Exception as err2:
                 raise err2 from err1
 
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        if not isinstance(self.module, nn.Module):
+            state["module"] = cloudpickle_dumps(state["module"])
+        return state
+
+    def __setstate__(self, state):
+        if "module" in state:
+            state["module"] = cloudpickle_loads(state["module"])
+        self.__dict__ = state
+
 
 class TensorDictModuleWrapper(TensorDictModuleBase):
     """Wrapper class for TensorDictModule objects.
 
     Once created, a TensorDictModuleWrapper will behave exactly as the
     TensorDictModule it contains except for the methods that are
     overwritten.
```

## tensordict/nn/functional_modules.py

```diff
@@ -196,23 +196,21 @@
             if not isinstance(batched_output, TensorDictBase):
                 out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
             else:
                 new_batch_size = list(batched_output.batch_size)
                 new_batch_size.insert(out_dim, batch_size)
                 new_names = list(batched_output.names)
                 new_names.insert(out_dim, None)
-                print(batched_output.names)
                 out = batched_output.apply(
                     lambda x, out_dim=out_dim: _remove_batch_dim(
                         x, vmap_level, batch_size, out_dim
                     ),
                     batch_size=new_batch_size,
                     names=new_names,
                 )
-                print(new_batch_size, out_dim, batch_size, flat_out_dims, out.names)
             flat_outputs.append(out)
         return tree_unflatten(flat_outputs, output_spec)
 
     vmap_src._unwrap_batched = _unwrap_batched
 
 
 # Tensordict-compatible Functional modules
```

## Comparing `tensordict_nightly-2023.5.9.dist-info/LICENSE` & `tensordict_nightly-2023.6.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2023.5.9.dist-info/METADATA` & `tensordict_nightly-2023.6.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2023.5.9
+Version: 2023.6.1
 Home-page: https://github.com/pytorch-labs/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
@@ -358,15 +358,15 @@
 
 If you're using TensorDict, please refer to this BibTeX entry to cite this work:
 ```
 @software{TensorDict,
   author = {Moens, Vincent},
   title = {{TensorDict: your PyTorch universal data carrier}},
   url = {https://github.com/pytorch-labs/tensordict},
-  version = {0.1.1},
+  version = {0.1.2},
   year = {2023}
 }
 ```
 
 ## Disclaimer
 
 TensorDict is at the *beta*-stage, meaning that there may be bc-breaking changes introduced, but
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.5.9 Home-page:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.6.1 Home-page:
 https://github.com/pytorch-labs/tensordict Author: tensordict contributors
 Author-email: vmoens@fb.com License: BSD Classifier: Programming Language ::
 Python :: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
 Programming Language :: Python :: 3.9 Classifier: Programming Language ::
 Python :: 3.10 Classifier: Development Status :: 4 - Beta Description-Content-
 Type: text/markdown License-File: LICENSE Requires-Dist: torch Requires-Dist:
 numpy Requires-Dist: cloudpickle Provides-Extra: checkpointing Requires-Dist:
@@ -181,13 +181,13 @@
 This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
 To enjoy the latest features, one can use ```bash pip install tensordict-
 nightly ``` **With Conda**: Install `tensordict` from `conda-forge` channel.
 ```sh conda install -c conda-forge tensordict ``` ## Citation If you're using
 TensorDict, please refer to this BibTeX entry to cite this work: ``` @software
 {TensorDict, author = {Moens, Vincent}, title = {{TensorDict: your PyTorch
 universal data carrier}}, url = {https://github.com/pytorch-labs/tensordict},
-version = {0.1.1}, year = {2023} } ``` ## Disclaimer TensorDict is at the
+version = {0.1.2}, year = {2023} } ``` ## Disclaimer TensorDict is at the
 *beta*-stage, meaning that there may be bc-breaking changes introduced, but
 they should come with a warranty. Hopefully these should not happen too often,
 as the current roadmap mostly involves adding new features and building
 compatibility with the broader PyTorch ecosystem. ## License TensorDict is
 licensed under the MIT License. See [LICENSE](LICENSE) for details.
```

## Comparing `tensordict_nightly-2023.5.9.dist-info/RECORD` & `tensordict_nightly-2023.6.1.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 tensordict/__init__.py,sha256=dDQidc208swLADInUxPlfvgfERJr2xeVTNC5NRvwN4A,1088
 tensordict/_contextlib.py,sha256=F4b0KH4FSeKsY9dkRuIXgHEn1dL_aNk7lFs1DonDYn4,6000
-tensordict/memmap.py,sha256=sBIRRJVadGiXlT5A8UfxoQqn6EEE1TBUDUaCyz65Yws,29601
-tensordict/persistent.py,sha256=CkF6hVsKAO21Le-7fuUChsLczoUr7Eflj6A6H7Z1XEo,33049
+tensordict/memmap.py,sha256=Xz3wV3JXiy-KWv49QHRSe3jeaG75Z8HZkcHA3g3jg8k,30428
+tensordict/persistent.py,sha256=ZbAID56GqmMFcuRfn77zBqrRVuKlGyE3H_YqWFJlOEU,33747
 tensordict/tensorclass.py,sha256=6eGhArep4GwEOeMFrdvv9MyVuy6oc4G58mhI2AVPh8M,30420
-tensordict/tensordict.py,sha256=lWNqG5grDyCjUU77Dws3XuFJn6AS62r9xFYtR_Q86bY,267149
+tensordict/tensordict.py,sha256=Y6-G7MxSp_eiuGC93b-Us04u0egMSv75LHghaNrLnr8,274343
 tensordict/utils.py,sha256=4ol71UgwHe9jvhiVG7-RH2fFFMWnRD1e8VdUY_g7Ffo,28030
-tensordict/version.py,sha256=tMxD31QWJi3KO4ITPBf3ko0INY1oQGVszQzzKvUbCJM,84
+tensordict/version.py,sha256=62gSnp_07cIfpYvmarOyG9BqYbcnnBg1XG5tjgGnoMw,84
 tensordict/nn/__init__.py,sha256=gjahPjJuC1uGPppYt7yZ8Ifusf2hTnOJHZGf8IutgoU,1314
-tensordict/nn/common.py,sha256=ncXtR3RsA3Ma5xxJILMi-W6or_hAmRwqZYsEaBxD208,35649
-tensordict/nn/functional_modules.py,sha256=jGJOUTaqCKO6Pmp8IuzGLWvej5WIOV26akH_0AMU7FI,19188
+tensordict/nn/common.py,sha256=uFaa3faPBV6BAdztpaAb27j7KtsutXYDyAbO_BGx1r0,37534
+tensordict/nn/functional_modules.py,sha256=LjMtRNvf8umx6vbIk0ZlpoeGJl6NSsN3oXB1a-_y1VQ,19059
 tensordict/nn/probabilistic.py,sha256=ZYa7f2PzWqgwTw3TJAcd125HRpD4p6aPc1HD5o709WM,22301
 tensordict/nn/sequence.py,sha256=l5nuYu0Ja11IO9qWSlijvRZH4XDLbUy8lwkFmyAHrHE,19564
 tensordict/nn/utils.py,sha256=Hb8zIWL_wGs8OSaGjqQJ4DAadLqA9d4GARR7EboKOFU,10621
 tensordict/nn/distributions/__init__.py,sha256=sD3yMuMceDJ8EqtptE4HEaS9Q_3keskCB9npPNFKXUo,499
 tensordict/nn/distributions/continuous.py,sha256=Y31G7ozOnlGUgzdqYZaKeXVjDOVMoSW1hIQWtcbyDO0,7073
 tensordict/nn/distributions/discrete.py,sha256=gUzTKs3yO8GkSY0ghBWVaZdHdmFyMMLWd5LV52CfrXY,2580
 tensordict/nn/distributions/truncated_normal.py,sha256=d1ontYbG_Q-W5gcVnWLadvZ1OBBeZGmBb5EUmr3ekak,6504
 tensordict/nn/distributions/utils.py,sha256=fX6NUeNnWKK6kDdOp8NTM43Lls3vMCF5h-S2teVZw3E,1226
 tensordict/prototype/__init__.py,sha256=b9Wmi1tbh2Em_wmKa52IXwxuudd6CUqdLgaob0bSWqQ,381
 tensordict/prototype/fx.py,sha256=Z3X821miRKcrOYwApvBDYt2_hST3IXNVe4v03tVvQKo,7507
 tensordict/prototype/tensorclass.py,sha256=pE53rWFqcPmuJOyOkI7cBZL9sMVfPx8bdQR1hNgYPvU,739
-tensordict_nightly-2023.5.9.dist-info/LICENSE,sha256=xdjS4_xk-IwnLuIFCvTYTl9Y8aXRejqpmke3dGam_nI,1098
-tensordict_nightly-2023.5.9.dist-info/METADATA,sha256=VieDhAtQlwKCaeEn0gwgBXM8yptiO6SOLzxsl3NHulI,15208
-tensordict_nightly-2023.5.9.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-tensordict_nightly-2023.5.9.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2023.5.9.dist-info/RECORD,,
+tensordict_nightly-2023.6.1.dist-info/LICENSE,sha256=xdjS4_xk-IwnLuIFCvTYTl9Y8aXRejqpmke3dGam_nI,1098
+tensordict_nightly-2023.6.1.dist-info/METADATA,sha256=coyZSNjtYHNe5PMYtiGEryN6xHhvJSiSEAznRL9phVI,15208
+tensordict_nightly-2023.6.1.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+tensordict_nightly-2023.6.1.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2023.6.1.dist-info/RECORD,,
```

