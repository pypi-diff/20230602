# Comparing `tmp/trulens_eval-0.1.1a0-py3-none-any.whl.zip` & `tmp/trulens_eval-0.1.2a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,30 @@
-Zip file size: 61753 bytes, number of entries: 23
--rw-r--r--  2.0 unx     4782 b- defN 23-May-23 22:00 trulens_eval/Example_TruBot.py
--rw-r--r--  2.0 unx     2422 b- defN 23-May-23 22:00 trulens_eval/Leaderboard.py
--rw-r--r--  2.0 unx      407 b- defN 23-May-24 13:14 trulens_eval/__init__.py
--rw-r--r--  2.0 unx     5401 b- defN 23-May-23 22:00 trulens_eval/benchmark.py
--rw-r--r--  2.0 unx     3443 b- defN 23-May-23 22:00 trulens_eval/feedback_prompts.py
--rw-r--r--  2.0 unx      949 b- defN 23-May-23 22:00 trulens_eval/keys.py
--rw-r--r--  2.0 unx     3325 b- defN 23-May-23 22:00 trulens_eval/provider_apis.py
--rw-r--r--  2.0 unx    12097 b- defN 23-May-23 22:00 trulens_eval/slackbot.py
--rw-r--r--  2.0 unx     5455 b- defN 23-May-23 22:00 trulens_eval/test_tru_chain.py
--rw-r--r--  2.0 unx    10400 b- defN 23-May-23 22:00 trulens_eval/tru.py
--rw-r--r--  2.0 unx    21491 b- defN 23-May-23 22:00 trulens_eval/tru_chain.py
--rw-r--r--  2.0 unx    27984 b- defN 23-May-23 22:00 trulens_eval/tru_db.py
--rw-r--r--  2.0 unx    30260 b- defN 23-May-23 22:00 trulens_eval/tru_feedback.py
--rw-r--r--  2.0 unx     3478 b- defN 23-May-23 22:00 trulens_eval/util.py
--rw-r--r--  2.0 unx     8618 b- defN 23-May-23 22:00 trulens_eval/pages/Evaluations.py
--rw-r--r--  2.0 unx     1322 b- defN 23-May-23 22:00 trulens_eval/pages/Progress.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-23 22:00 trulens_eval/pages/__init__.py
--rw-r--r--  2.0 unx      929 b- defN 23-May-23 22:00 trulens_eval/ux/add_logo.py
--rw-r--r--  2.0 unx    29567 b- defN 23-May-23 22:00 trulens_eval/ux/trulens_logo.svg
--rw-r--r--  2.0 unx    12698 b- defN 23-May-24 13:15 trulens_eval-0.1.1a0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-24 13:15 trulens_eval-0.1.1a0.dist-info/WHEEL
--rw-r--r--  2.0 unx       13 b- defN 23-May-24 13:15 trulens_eval-0.1.1a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1917 b- defN 23-May-24 13:15 trulens_eval-0.1.1a0.dist-info/RECORD
-23 files, 187050 bytes uncompressed, 58657 bytes compressed:  68.6%
+Zip file size: 71620 bytes, number of entries: 28
+-rw-rw-r--  2.0 unx     4782 b- defN 23-Jun-02 15:56 trulens_eval/Example_TruBot.py
+-rw-rw-r--  2.0 unx     2422 b- defN 23-Jun-02 15:56 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx      407 b- defN 23-Jun-02 16:01 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx     5401 b- defN 23-Jun-02 15:56 trulens_eval/benchmark.py
+-rw-rw-r--  2.0 unx     3443 b- defN 23-Jun-02 15:56 trulens_eval/feedback_prompts.py
+-rw-rw-r--  2.0 unx      949 b- defN 23-Jun-02 15:56 trulens_eval/keys.py
+-rw-rw-r--  2.0 unx     3341 b- defN 23-Jun-02 15:56 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    11179 b- defN 23-Jun-02 15:56 trulens_eval/slackbot.py
+-rw-rw-r--  2.0 unx     5455 b- defN 23-Jun-02 15:56 trulens_eval/test_tru_chain.py
+-rw-rw-r--  2.0 unx    10449 b- defN 23-Jun-02 15:56 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx    22796 b- defN 23-Jun-02 15:56 trulens_eval/tru_chain.py
+-rw-rw-r--  2.0 unx    29397 b- defN 23-Jun-02 15:56 trulens_eval/tru_db.py
+-rw-rw-r--  2.0 unx    30628 b- defN 23-Jun-02 15:56 trulens_eval/tru_feedback.py
+-rw-rw-r--  2.0 unx     4067 b- defN 23-Jun-02 15:56 trulens_eval/util.py
+-rw-rw-r--  2.0 unx     4782 b- defN 23-Jun-02 15:56 trulens_eval/examples/App_TruBot.py
+-rw-rw-r--  2.0 unx    11074 b- defN 23-Jun-02 15:56 trulens_eval/examples/trubot.py
+-rw-rw-r--  2.0 unx    10278 b- defN 23-Jun-02 15:56 trulens_eval/pages/Evaluations.py
+-rw-rw-r--  2.0 unx     1323 b- defN 23-Jun-02 15:56 trulens_eval/pages/Progress.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-02 15:56 trulens_eval/pages/__init__.py
+-rw-rw-r--  2.0 unx     5455 b- defN 23-Jun-02 15:56 trulens_eval/tests/test_tru_chain.py
+-rw-rw-r--  2.0 unx     1484 b- defN 23-Jun-02 15:56 trulens_eval/utils/langchain.py
+-rw-rw-r--  2.0 unx      915 b- defN 23-Jun-02 15:56 trulens_eval/ux/add_logo.py
+-rw-rw-r--  2.0 unx     1142 b- defN 23-Jun-02 15:56 trulens_eval/ux/components.py
+-rw-rw-r--  2.0 unx    29567 b- defN 23-Jun-02 15:56 trulens_eval/ux/trulens_logo.svg
+-rw-rw-r--  2.0 unx    12734 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2366 b- defN 23-Jun-02 16:01 trulens_eval-0.1.2a0.dist-info/RECORD
+28 files, 215941 bytes uncompressed, 67820 bytes compressed:  68.6%
```

## zipnote {}

```diff
@@ -36,35 +36,50 @@
 
 Filename: trulens_eval/tru_feedback.py
 Comment: 
 
 Filename: trulens_eval/util.py
 Comment: 
 
+Filename: trulens_eval/examples/App_TruBot.py
+Comment: 
+
+Filename: trulens_eval/examples/trubot.py
+Comment: 
+
 Filename: trulens_eval/pages/Evaluations.py
 Comment: 
 
 Filename: trulens_eval/pages/Progress.py
 Comment: 
 
 Filename: trulens_eval/pages/__init__.py
 Comment: 
 
+Filename: trulens_eval/tests/test_tru_chain.py
+Comment: 
+
+Filename: trulens_eval/utils/langchain.py
+Comment: 
+
 Filename: trulens_eval/ux/add_logo.py
 Comment: 
 
+Filename: trulens_eval/ux/components.py
+Comment: 
+
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.1.1a0.dist-info/METADATA
+Filename: trulens_eval-0.1.2a0.dist-info/METADATA
 Comment: 
 
-Filename: trulens_eval-0.1.1a0.dist-info/WHEEL
+Filename: trulens_eval-0.1.2a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.1.1a0.dist-info/top_level.txt
+Filename: trulens_eval-0.1.2a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.1.1a0.dist-info/RECORD
+Filename: trulens_eval-0.1.2a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/__init__.py

```diff
@@ -1,12 +1,12 @@
 """
 Imports of most common parts of the library. Should include everything to get started.
 """
 
-__version__ = "0.1.1a"
+__version__ = "0.1.2a"
 
 from trulens_eval.tru_chain import TruChain
 from trulens_eval.tru_feedback import Feedback
 from trulens_eval.tru_feedback import OpenAI
 from trulens_eval.tru_feedback import Huggingface
 from trulens_eval.tru import Tru
```

## trulens_eval/provider_apis.py

```diff
@@ -55,29 +55,31 @@
         Block until we can make a request to this endpoint.
         """
 
         self.pace.get()
         self.tqdm.update(1)
         return
 
-    def post(self, url: str, payload: JSON, timeout: Optional[int] = None) -> Any:
+    def post(
+        self, url: str, payload: JSON, timeout: Optional[int] = None
+    ) -> Any:
         extra = dict()
         if self.post_headers is not None:
             extra['headers'] = self.post_headers
 
         self.pace_me()
         ret = requests.post(url, json=payload, timeout=timeout, **extra)
 
         j = ret.json()
 
         # Huggingface public api sometimes tells us that a model is loading and how long to wait:
         if "estimated_time" in j:
             wait_time = j['estimated_time']
             logging.error(f"Waiting for {j} ({wait_time}) second(s).")
-            sleep(wait_time+2)
+            sleep(wait_time + 2)
             return self.post(url, payload)
 
         assert isinstance(
             j, Sequence
         ) and len(j) > 0, f"Post did not return a sequence: {j}"
 
         return j[0]
```

## trulens_eval/slackbot.py

```diff
@@ -4,19 +4,15 @@
 from typing import Callable, Dict, List, Set, Tuple
 
 import numpy as np
 
 # This needs to be before some others to make sure api keys are ready before
 # relevant classes are loaded.
 from trulens_eval.keys import *
-
-
-# This is here so that import organizer does not move the keys import below this
-# line.
-_ = None
+"This is here so that import organizer does not move the keys import below this"
 
 from langchain.chains import ConversationalRetrievalChain
 from langchain.embeddings.openai import OpenAIEmbeddings
 from langchain.llms import OpenAI
 from langchain.memory import ConversationSummaryBufferMemory
 from langchain.schema import Document
 from langchain.vectorstores import Pinecone
@@ -29,14 +25,15 @@
 from trulens_eval import Tru
 from trulens_eval import tru_feedback
 from trulens_eval.tru_chain import TruChain
 from trulens_eval.tru_db import LocalSQLite
 from trulens_eval.tru_db import Record
 from trulens_eval.tru_feedback import Feedback
 from trulens_eval.util import TP
+from trulens_eval.utils.langchain import WithFilterDocuments
 
 os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
 
 pp = PrettyPrinter()
 
 PORT = 3000
 verb = False
@@ -54,15 +51,15 @@
 convos: Dict[str, TruChain] = dict()
 
 # Keep track of timestamps of messages already handled. Sometimes the same
 # message gets received more than once if there is a network hickup.
 handled_ts: Set[Tuple[str, str]] = set()
 
 # DB to save models and records.
-tru = Tru()#LocalSQLite("trubot.sqlite"))
+tru = Tru()  #LocalSQLite("trubot.sqlite"))
 
 ident = lambda h: h
 
 chain_ids = {
     0: "0/default",
     1: "1/lang_prompt",
     2: "2/relevance_prompt",
@@ -89,45 +86,19 @@
 f_qs_relevance = Feedback(openai.qs_relevance).on(
     question="input",
     statement=Record.chain.combine_docs_chain._call.args.inputs.input_documents
 ).on_multiple(
     multiarg="statement", each_query=Record.page_content, agg=np.min
 )
 
-class WithFilterDocuments(VectorStoreRetriever):
-    filter_func: Callable = Field(exclude=True)
-
-    def __init__(self, filter_func: Callable, *args, **kwargs):
-        super().__init__(filter_func=filter_func, *args, **kwargs)
-        # self.filter_func = filter_func
-
-    def get_relevant_documents(self, query: str) -> List[Document]:
-        docs = super().get_relevant_documents(query)
-
-        promises = []
-        for doc in docs:
-            promises.append(
-                (doc, TP().promise(self.filter_func, query=query, doc=doc))
-            )
-
-        results = []
-        for doc, promise in promises:
-            results.append((doc, promise.get()))
-
-        docs_filtered = map(lambda sr: sr[0], filter(lambda sr: sr[1], results))
-
-        return list(docs_filtered)
-
-    @staticmethod
-    def of_vectorstoreretriever(retriever, filter_func: Callable):
-        return WithFilterDocuments(filter_func=filter_func, **retriever.dict())
 
 def filter_by_relevance(query, doc):
     return openai.qs_relevance(question=query, statement=doc.page_content) > 0.5
 
+
 def get_or_make_chain(cid: str, selector: int = 0) -> TruChain:
     """
     Create a new chain for the given conversation id `cid` or return an existing
     one. Return the new or existing chain.
     """
 
     # NOTE(piotrm): Unsure about the thread safety of the various components so
@@ -148,15 +119,15 @@
     docsearch = Pinecone.from_existing_index(
         index_name="llmdemo", embedding=embedding
     )
 
     retriever = docsearch.as_retriever()
 
     if "filtered" in chain_id:
-        retriever = WithFilterDocuments.of_vectorstoreretriever(
+        retriever = WithFilterDocuments.of_retriever(
             retriever=retriever, filter_func=filter_by_relevance
         )
 
     # LLM for completing prompts, and other tasks.
     llm = OpenAI(temperature=0, max_tokens=128)
 
     # Conversation memory.
```

## trulens_eval/tru.py

```diff
@@ -235,22 +235,25 @@
         from trulens_eval.tru_feedback import Feedback
 
         if not fork:
             self.evaluator_stop = threading.Event()
 
         def runloop():
             while fork or not self.evaluator_stop.is_set():
-                print("Looking for things to do. Stop me with `tru.stop_evaluator()`.", end='')
+                print(
+                    "Looking for things to do. Stop me with `tru.stop_evaluator()`.",
+                    end=''
+                )
                 Feedback.evaluate_deferred(tru=self)
                 TP().finish(timeout=10)
                 if fork:
                     sleep(10)
                 else:
                     self.evaluator_stop.wait(10)
-                
+
             print("Evaluator stopped.")
 
         if fork:
             proc = Process(target=runloop)
         else:
             proc = Thread(target=runloop)
 
@@ -264,49 +267,51 @@
     def stop_evaluator(self):
         """
         Stop the deferred feedback evaluation thread.
         """
 
         if self.evaluator_proc is None:
             raise RuntimeError("Evaluator not running this process.")
-        
+
         if isinstance(self.evaluator_proc, Process):
             self.evaluator_proc.terminate()
 
         elif isinstance(self.evaluator_proc, Thread):
             self.evaluator_stop.set()
             self.evaluator_proc.join()
             self.evaluator_stop = None
-            
+
         self.evaluator_proc = None
-        
+
     def stop_dashboard(self) -> None:
         """Stop existing dashboard if running.
 
         Raises:
             ValueError: Dashboard is already running.
         """
         if Tru.dashboard_proc is None:
             raise ValueError("Dashboard not running.")
-        
+
         Tru.dashboard_proc.kill()
         Tru.dashboard_proc = None
 
     def run_dashboard(self, _dev: bool = False) -> Process:
         """ Runs a streamlit dashboard to view logged results and chains
 
         Raises:
             ValueError: Dashboard is already running.
 
         Returns:
             Process: Process containing streamlit dashboard.
         """
 
         if Tru.dashboard_proc is not None:
-            raise ValueError("Dashboard already running. Run tru.stop_dashboard() to stop existing dashboard.")
+            raise ValueError(
+                "Dashboard already running. Run tru.stop_dashboard() to stop existing dashboard."
+            )
 
         # Create .streamlit directory if it doesn't exist
         streamlit_dir = os.path.join(os.getcwd(), '.streamlit')
         os.makedirs(streamlit_dir, exist_ok=True)
 
         # Create config.toml file
         config_path = os.path.join(streamlit_dir, 'config.toml')
@@ -330,15 +335,16 @@
 
         env_opts = {}
         if _dev:
             env_opts['env'] = os.environ
             env_opts['env']['PYTHONPATH'] = str(Path.cwd())
 
         proc = subprocess.Popen(
-            ["streamlit", "run", "--server.headless=True", leaderboard_path], **env_opts
+            ["streamlit", "run", "--server.headless=True", leaderboard_path],
+            **env_opts
         )
 
         Tru.dashboard_proc = proc
 
         return proc
 
-    start_dashboard = run_dashboard
+    start_dashboard = run_dashboard
```

## trulens_eval/tru_chain.py

```diff
@@ -70,14 +70,15 @@
 """
 
 from collections import defaultdict
 from datetime import datetime
 from inspect import BoundArguments
 from inspect import signature
 from inspect import stack
+import inspect
 import logging
 import os
 import threading as th
 from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 import langchain
 from langchain.callbacks import get_openai_callback
@@ -199,15 +200,14 @@
                     "`tru` is specified but `feedback_mode` is None. "
                     "No feedback evaluation and logging will occur."
                 )
         else:
             if feedback_mode is not None:
                 logging.warn(
                     f"`feedback_mode` is {feedback_mode} but `tru` was not specified. Reverting to None."
-
                 )
                 self.feedback_mode = None
                 feedback_mode = None
                 # Import here to avoid circular imports.
                 # from trulens_eval import Tru
                 # tru = Tru()
 
@@ -300,15 +300,15 @@
         ret_record['_cost'] = dict(
             total_tokens=total_tokens, total_cost=total_cost
         )
         ret_record['chain_id'] = self.chain_id
 
         if error is not None:
 
-            if self.feedback_mode == "withchain":            
+            if self.feedback_mode == "withchain":
                 self._handle_error(record_json=ret_record, error=error)
 
             elif self.feedback_mode in ["deferred", "withchainthread"]:
                 TP().runlater(
                     self._handle_error, record_json=ret_record, error=error
                 )
 
@@ -360,15 +360,15 @@
         # Add empty (to run) feedback to db.
         if self.feedback_mode == "deferred":
             for f in self.feedbacks:
                 feedback_id = f.feedback_id
                 self.db.insert_feedback(record_id, feedback_id)
 
         elif self.feedback_mode in ["withchain", "withchainthread"]:
-            
+
             results = self.tru.run_feedback_functions(
                 record_json=record_json,
                 feedback_functions=self.feedbacks,
                 chain_json=self.json
             )
 
             for result_json in results:
@@ -407,34 +407,38 @@
 
     def _instrument_dict(self, cls, obj: Any):
         """
         Replacement for langchain's dict method to one that does not fail under
         non-serialization situations.
         """
 
-        if obj.memory is not None:
-
-            # logging.warn(
-            #     f"Will not be able to serialize object of type {cls} because it has memory."
-            # )
-
-            pass
+        if hasattr(obj, "memory"):
+            if obj.memory is not None:
+                # logging.warn(
+                #     f"Will not be able to serialize object of type {cls} because it has memory."
+                # )
+                pass
 
         def safe_dict(s, json: bool = True, **kwargs: Any) -> Dict:
             """
             Return dictionary representation `s`. If `json` is set, will make
             sure output can be serialized.
             """
 
             #if s.memory is not None:
             # continue anyway
             # raise ValueError("Saving of memory is not yet supported.")
 
-            _dict = super(cls, s).dict(**kwargs)
-            _dict["_type"] = s._chain_type
+            sup = super(cls, s)
+            if hasattr(sup, "dict"):
+                _dict = super(cls, s).dict(**kwargs)
+            else:
+                _dict = {"_base_type": cls.__name__}
+            # _dict = cls.dict(s, **kwargs)
+            # _dict["_type"] = s._chain_type
 
             # TODO: json
 
             return _dict
 
         safe_dict._instrumented = getattr(cls, "dict")
 
@@ -463,19 +467,25 @@
                 return noserio(obj, error=f"{e.__class__.__name__}='{str(e)}'")
 
         safe_type._instrumented = prop
         new_prop = property(fget=safe_type)
 
         return new_prop
 
-    def _instrument_call(self, query: Query, func: Callable):
+    def _instrument_tracked_method(
+        self, query: Query, func: Callable, method_name: str, class_name: str,
+        module_name: str
+    ):
         """
-        Instrument a Chain.__call__ method to capture its inputs/outputs/errors.
+        Instrument a method to capture its inputs/outputs/errors.
         """
 
+        if self.verbose:
+            print(f"instrumenting {method_name}={func} in {query._path}")
+
         if hasattr(func, "_instrumented"):
             if self.verbose:
                 print(f"{func} is already instrumented")
 
             # Already instrumented. Note that this may happen under expected
             # operation when the same chain is used multiple times as part of a
             # larger chain.
@@ -510,16 +520,22 @@
             error = None
             ret = None
 
             start_time = datetime.now()
 
             chain_stack = self._get_local_in_call_stack(
                 key="chain_stack", func=wrapper, offset=1
-            ) or []
-            chain_stack = chain_stack + [query._path]
+            ) or ()
+            frame_ident = dict(
+                path=tuple(query._path),
+                method_name=method_name,
+                class_name=class_name,
+                module_name=module_name,
+            )
+            chain_stack = chain_stack + (frame_ident,)
 
             try:
                 # Using sig bind here so we can produce a list of key-value
                 # pairs even if positional arguments were provided.
                 bindings: BoundArguments = sig.bind(*args, **kwargs)
                 ret = func(*bindings.args, **bindings.kwargs)
 
@@ -530,14 +546,15 @@
 
             # Don't include self in the recorded arguments.
             nonself = {
                 k: TruDB.jsonify(v)
                 for k, v in bindings.arguments.items()
                 if k != "self"
             }
+
             row_args = dict(
                 args=nonself,
                 start_time=str(start_time),
                 end_time=str(end_time),
                 pid=os.getpid(),
                 tid=th.get_native_id(),
                 chain_stack=chain_stack
@@ -572,41 +589,57 @@
         # deceptive if the same subchain appears multiple times in the wrapped
         # chain.
         wrapper._query = query
 
         return wrapper
 
     def _instrument_object(self, obj, query: Query):
-        if self.verbose:
-            print(f"instrumenting {query._path} {obj.__class__.__name__}")
 
-        cls = obj.__class__
+        # cls = inspect.getattr_static(obj, "__class__").__get__()
+        cls = type(obj)
+
+        if self.verbose:
+            pass
+            #print(
+            #    f"instrumenting {query._path} {cls.__name__}, bases={cls.__bases__}"
+            #)
 
         # NOTE: We cannot instrument chain directly and have to instead
         # instrument its class. The pydantic BaseModel does not allow instance
         # attributes that are not fields:
         # https://github.com/pydantic/pydantic/blob/11079e7e9c458c610860a5776dc398a4764d538d/pydantic/main.py#LL370C13-L370C13
         # .
-        for base in cls.mro():
+
+        methods_to_instrument = {"_call", "get_relevant_documents"}
+
+        for base in [cls] + cls.mro():
             # All of mro() may need instrumentation here if some subchains call
             # superchains, and we want to capture the intermediate steps.
 
-            if not base.__module__.startswith("langchain."):
+            if not base.__module__.startswith(
+                    "langchain.") and not base.__module__.startswith("trulens"):
                 continue
 
-            if hasattr(base, "_call"):
-                original_fun = getattr(base, "_call")
-
-                if self.verbose:
-                    print(f"instrumenting {base}._call")
-
-                setattr(
-                    base, "_call",
-                    self._instrument_call(query=query, func=original_fun)
-                )
+            for method_name in methods_to_instrument:
+                if hasattr(base, method_name):
+                    original_fun = getattr(base, method_name)
+
+                    if self.verbose:
+                        print(f"instrumenting {base}.{method_name}")
+
+                    setattr(
+                        base, method_name,
+                        self._instrument_tracked_method(
+                            query=query,
+                            func=original_fun,
+                            method_name=method_name,
+                            class_name=base.__name__,
+                            module_name=base.__module__
+                        )
+                    )
 
             if hasattr(base, "_chain_type"):
                 if self.verbose:
                     print(f"instrumenting {base}._chain_type")
 
                 prop = getattr(base, "_chain_type")
                 setattr(
@@ -620,15 +653,15 @@
 
                 prop = getattr(base, "_prompt_type")
                 setattr(
                     base, "_prompt_type",
                     self._instrument_type_method(obj=obj, prop=prop)
                 )
 
-            if isinstance(obj, Chain):
+            if hasattr(base, "dict"):
                 if self.verbose:
                     print(f"instrumenting {base}.dict")
 
                 setattr(base, "dict", self._instrument_dict(cls=base, obj=obj))
 
         # Not using chain.dict() here as that recursively converts subchains to
         # dicts but we want to traverse the instantiations here.
@@ -637,20 +670,21 @@
             for k in obj.__fields__:
                 # NOTE(piotrm): may be better to use inspect.getmembers_static .
                 v = getattr(obj, k)
 
                 if isinstance(v, str):
                     pass
 
-                elif v.__class__.__module__.startswith("langchain."):
+                elif type(v).__module__.startswith("langchain.") or type(
+                        v).__module__.startswith("trulens"):
                     self._instrument_object(obj=v, query=query[k])
 
                 elif isinstance(v, Sequence):
                     for i, sv in enumerate(v):
                         if isinstance(sv, Chain):
                             self._instrument_object(obj=sv, query=query[k][i])
 
                 # TODO: check if we want to instrument anything not accessible through __fields__ .
         else:
             logging.debug(
-                f"Do not know how to instrument object {str(obj)[:32]} of type {type(obj)}."
+                f"Do not know how to instrument object {str(obj)[:32]} of type {cls}."
             )
```

## trulens_eval/tru_db.py

```diff
@@ -3,30 +3,34 @@
 import logging
 from pathlib import Path
 import sqlite3
 from typing import (
     Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple, Union
 )
 
+from frozendict import frozendict
 from merkle_json import MerkleJson
 import pandas as pd
 import pydantic
 from tinydb import Query as TinyQuery
 from tinydb.queries import QueryInstance as TinyQueryInstance
-from trulens_eval.util import UNCIODE_YIELD, UNICODE_CHECK
+
+from trulens_eval.util import UNCIODE_YIELD
+from trulens_eval.util import UNICODE_CHECK
 
 mj = MerkleJson()
 NoneType = type(None)
 
 JSON_BASES = (str, int, float, NoneType)
 JSON_BASES_T = Union[str, int, float, NoneType]
 # JSON = (List, Dict) + JSON_BASES
 # JSON_T = Union[JSON_BASES_T, List, Dict]
 JSON = Dict
 
+
 def is_empty(obj):
     try:
         return len(obj) == 0
     except Exception:
         return False
 
 
@@ -97,15 +101,56 @@
 # Instance for constructing queries for chain json.
 Chain = Query()._chain
 
 # Type of conditions, constructed from query/record like `Record.chain != None`.
 Condition = TinyQueryInstance
 
 
+def get_calls(record_json: JSON) -> Iterable[JSON]:
+    """
+    Iterate over the call parts of the record.
+    """
+
+    for q in TruDB.all_queries(record_json):
+        if q._path[-1] == "_call":
+            yield q
+
+
+def get_calls_by_stack(record_json: JSON) -> Dict[Tuple[str, ...], JSON]:
+    """
+    Get a dictionary mapping chain call stack to the call information.
+    """
+
+    def frozen_frame(frame):
+        frame['path'] = tuple(frame['path'])
+        return frozendict(frame)
+
+    ret = dict()
+    for c in get_calls(record_json):
+        obj = TruDB.project(c, record_json=record_json, chain_json=None)
+        if isinstance(obj, Sequence):
+            for o in obj:
+                call_stack = tuple(map(frozen_frame, o['chain_stack']))
+                if call_stack not in ret:
+                    ret[call_stack] = []
+                ret[call_stack].append(o)
+        else:
+            call_stack = tuple(map(frozen_frame, obj['chain_stack']))
+            if call_stack not in ret:
+                ret[call_stack] = []
+            ret[call_stack].append(obj)
+
+    return ret
+
+
 def query_of_path(path: List[Union[str, int]]) -> Query:
+    """
+    Convert the given path to a query object.
+    """
+
     if path[0] == "_record":
         ret = Record
         path = path[1:]
     elif path[0] == "_chain":
         ret = Chain
         path = path[1:]
     else:
@@ -545,26 +590,18 @@
         self._clear_tables()
         self._build_tables()
 
     def _clear_tables(self):
         conn, c = self._connect()
 
         # Create table if it does not exist
-        c.execute(
-            f'''DELETE FROM {self.TABLE_RECORDS}'''
-        )
-        c.execute(
-            f'''DELETE FROM {self.TABLE_FEEDBACKS}'''
-        )
-        c.execute(
-            f'''DELETE FROM {self.TABLE_FEEDBACK_DEFS}'''
-        )
-        c.execute(
-            f'''DELETE FROM {self.TABLE_CHAINS}'''
-        )
+        c.execute(f'''DELETE FROM {self.TABLE_RECORDS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_FEEDBACKS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_FEEDBACK_DEFS}''')
+        c.execute(f'''DELETE FROM {self.TABLE_CHAINS}''')
         self._close(conn)
 
     def _build_tables(self):
         conn, c = self._connect()
 
         # Create table if it does not exist
         c.execute(
@@ -640,15 +677,17 @@
             (
                 record_id, chain_id, input, output, record_str, tags, ts,
                 total_tokens, total_cost
             )
         )
         self._close(conn)
 
-        print(f"{UNICODE_CHECK} record {record_id} from {chain_id} -> {self.filename}")
+        print(
+            f"{UNICODE_CHECK} record {record_id} from {chain_id} -> {self.filename}"
+        )
 
         return record_id
 
     # TruDB requirement
     def insert_chain(
         self, chain_json: dict, chain_id: Optional[str] = None
     ) -> str:
@@ -748,17 +787,21 @@
                 record_id, feedback_id, last_ts, status, result_str,
                 total_tokens, total_cost
             )
         )
         self._close(conn)
 
         if status == 2:
-            print(f"{UNICODE_CHECK} feedback {feedback_id} on {record_id} -> {self.filename}")
+            print(
+                f"{UNICODE_CHECK} feedback {feedback_id} on {record_id} -> {self.filename}"
+            )
         else:
-            print(f"{UNCIODE_YIELD} feedback {feedback_id} on {record_id} -> {self.filename}")
+            print(
+                f"{UNCIODE_YIELD} feedback {feedback_id} on {record_id} -> {self.filename}"
+            )
 
     def get_feedback(
         self,
         record_id: Optional[str] = None,
         feedback_id: Optional[str] = None,
         status: Optional[int] = None,
         last_ts_before: Optional[int] = None
@@ -859,15 +902,17 @@
             (chain_id,)
         )
         result = c.fetchone()[0]
         conn.close()
 
         return json.loads(result)
 
-    def get_records_and_feedback(self, chain_ids: List[str]) -> Tuple[pd.DataFrame, Sequence[str]]:
+    def get_records_and_feedback(
+        self, chain_ids: List[str]
+    ) -> Tuple[pd.DataFrame, Sequence[str]]:
         # This returns all models if the list of chain_ids is empty.
         conn, c = self._connect()
         query = f"""
             SELECT r.record_id, f.result_json
             FROM {self.TABLE_RECORDS} r 
             LEFT JOIN {self.TABLE_FEEDBACKS} f
                 ON r.record_id = f.record_id
@@ -897,38 +942,47 @@
         if len(chain_ids) > 0:
             chain_id_list = ', '.join('?' * len(chain_ids))
             query = query + f" WHERE r.chain_id IN ({chain_id_list})"
 
         c.execute(query)
         rows = c.fetchall()
         conn.close()
-        
+
         df_records = pd.DataFrame(
             rows, columns=[description[0] for description in c.description]
         )
-        
+
         if len(df_records) == 0:
             return df_records, []
 
         # Apply the function to the 'data' column to convert it into separate columns
-        df_results['result_json'] = df_results['result_json'].apply(lambda d: {} if d is None else json.loads(d)) 
+        df_results['result_json'] = df_results['result_json'].apply(
+            lambda d: {} if d is None else json.loads(d)
+        )
 
         if "record_id" not in df_results.columns:
             return df_results, []
 
         df_results = df_results.groupby("record_id").agg(
             lambda dicts: {key: val for d in dicts for key, val in d.items()}
         ).reset_index()
-        
+
         df_results = df_results['result_json'].apply(pd.Series)
 
-        result_cols = [col for col in df_results.columns if col not in ['feedback_id', 'record_id', '_success', "_error"]]
-        
+        result_cols = [
+            col for col in df_results.columns
+            if col not in ['feedback_id', 'record_id', '_success', "_error"]
+        ]
+
         if len(df_results) == 0 or len(result_cols) == 0:
             return df_records, []
 
         assert "record_id" in df_results.columns
         assert "record_id" in df_records.columns
 
         combined_df = df_records.merge(df_results, on=['record_id'])
+        combined_df = combined_df.drop(
+            columns=set(["_success", "_error"]
+                       ).intersection(set(combined_df.columns))
+        )
 
         return combined_df, result_cols
```

## trulens_eval/tru_feedback.py

```diff
@@ -134,15 +134,17 @@
             assert hasattr(
                 imp, "__self__"
             ), "Feedback implementation is not a method (it may be a function)."
             self.provider = imp.__self__
             check_provider(self.provider.__class__.__name__)
             self.imp_method_name = imp.__name__
             self._json = self.to_json()
-            self._feedback_id = feedback_id or obj_id_of_obj(self._json, prefix="feedback")
+            self._feedback_id = feedback_id or obj_id_of_obj(
+                self._json, prefix="feedback"
+            )
             self._json['feedback_id'] = self._feedback_id
 
     @staticmethod
     def evaluate_deferred(tru: 'Tru'):
         db = tru.db
 
         def prepare_feedback(row):
@@ -157,41 +159,53 @@
             if row.status == 0:
                 tqdm.write(f"Starting run for row {i}.")
 
                 TP().runlater(prepare_feedback, row)
             elif row.status in [1]:
                 now = datetime.now().timestamp()
                 if now - row.last_ts > 30:
-                    tqdm.write(f"Incomplete row {i} last made progress over 30 seconds ago. Retrying.")
+                    tqdm.write(
+                        f"Incomplete row {i} last made progress over 30 seconds ago. Retrying."
+                    )
                     TP().runlater(prepare_feedback, row)
                 else:
-                    tqdm.write(f"Incomplete row {i} last made progress less than 30 seconds ago. Giving it more time.")
+                    tqdm.write(
+                        f"Incomplete row {i} last made progress less than 30 seconds ago. Giving it more time."
+                    )
 
             elif row.status in [-1]:
                 now = datetime.now().timestamp()
-                if now - row.last_ts > 60*5:
-                    tqdm.write(f"Failed row {i} last made progress over 5 minutes ago. Retrying.")
+                if now - row.last_ts > 60 * 5:
+                    tqdm.write(
+                        f"Failed row {i} last made progress over 5 minutes ago. Retrying."
+                    )
                     TP().runlater(prepare_feedback, row)
                 else:
-                    tqdm.write(f"Failed row {i} last made progress less than 5 minutes ago. Not touching it for now.")
+                    tqdm.write(
+                        f"Failed row {i} last made progress less than 5 minutes ago. Not touching it for now."
+                    )
 
             elif row.status == 2:
                 pass
 
         # TP().finish()
         # TP().runrepeatedly(runner)
 
     @property
     def json(self):
-        assert hasattr(self, "_json"), "Cannot json-size partially defined feedback function."
+        assert hasattr(
+            self, "_json"
+        ), "Cannot json-size partially defined feedback function."
         return self._json
 
     @property
     def feedback_id(self):
-        assert hasattr(self, "_feedback_id"), "Cannot get id of partially defined feedback function."
+        assert hasattr(
+            self, "_feedback_id"
+        ), "Cannot get id of partially defined feedback function."
         return self._feedback_id
 
     @staticmethod
     def selection_to_json(select: Selection) -> dict:
         if isinstance(select, str):
             return select
         elif isinstance(select, Query):
@@ -266,29 +280,31 @@
             ), f"Feedback function expected a sequence on {multiarg} argument."
 
             rets: List[AsyncResult[float]] = []
 
             for aval in multi:
 
                 if each_query is not None:
-                    aval = TruDB.project(query=each_query, obj=aval)
+                    aval = TruDB.project(
+                        query=each_query, record_json=aval, chain_json=None
+                    )
 
                 kwargs[multiarg] = aval
 
                 rets.append(TP().promise(self.imp, **kwargs))
 
             rets: List[float] = list(map(lambda r: r.get(), rets))
 
             rets = np.array(rets)
 
             return agg(rets)
 
         wrapped_imp.__name__ = self.imp.__name__
 
-        wrapped_imp.__self__ = self.imp.__self__ # needed for serialization
+        wrapped_imp.__self__ = self.imp.__self__  # needed for serialization
 
         # Copy over signature from wrapped function. Otherwise signature of the
         # wrapped method will include just kwargs which is insufficient for
         # verify arguments (see Feedback.__init__).
         wrapped_imp.__signature__ = signature(self.imp)
 
         return Feedback(imp=wrapped_imp, selectors=self.selectors)
@@ -327,51 +343,55 @@
         names.
         """
 
         if 'record_id' not in record_json:
             record_json['record_id'] = None
 
         try:
-            ins = self.extract_selection(chain_json=chain_json, record_json=record_json)
+            ins = self.extract_selection(
+                chain_json=chain_json, record_json=record_json
+            )
             ret = self.imp(**ins)
-            
+
             return {
                 '_success': True,
                 'feedback_id': self.feedback_id,
                 'record_id': record_json['record_id'],
                 self.name: ret
             }
-        
+
         except Exception as e:
             return {
                 '_success': False,
                 'feedback_id': self.feedback_id,
                 'record_id': record_json['record_id'],
                 '_error': str(e)
             }
 
     def run_and_log(self, record_json: JSON, tru: 'Tru') -> None:
         record_id = record_json['record_id']
         chain_id = record_json['chain_id']
-        
+
         ts_now = datetime.now().timestamp()
 
         db = tru.db
 
         try:
             db.insert_feedback(
                 record_id=record_id,
                 feedback_id=self.feedback_id,
-                last_ts = ts_now,
-                status = 1 # in progress
+                last_ts=ts_now,
+                status=1  # in progress
             )
 
             chain_json = db.get_chain(chain_id=chain_id)
 
-            res = self.run_on_record(chain_json=chain_json, record_json=record_json)
+            res = self.run_on_record(
+                chain_json=chain_json, record_json=record_json
+            )
 
         except Exception as e:
             print(e)
             res = {
                 '_success': False,
                 'feedback_id': self.feedback_id,
                 'record_id': record_json['record_id'],
@@ -380,46 +400,43 @@
 
         ts_now = datetime.now().timestamp()
 
         if res['_success']:
             db.insert_feedback(
                 record_id=record_id,
                 feedback_id=self.feedback_id,
-                last_ts = ts_now,
-                status = 2, # done and good
+                last_ts=ts_now,
+                status=2,  # done and good
                 result_json=res,
-                total_cost=-1.0, # todo
+                total_cost=-1.0,  # todo
                 total_tokens=-1  # todo
             )
         else:
             # TODO: indicate failure better
             db.insert_feedback(
                 record_id=record_id,
                 feedback_id=self.feedback_id,
-                last_ts = ts_now,
-                status = -1, # failure
+                last_ts=ts_now,
+                status=-1,  # failure
                 result_json=res,
-                total_cost=-1.0, # todo
+                total_cost=-1.0,  # todo
                 total_tokens=-1  # todo
             )
 
     @property
     def name(self):
         """
         Name of the feedback function. Presently derived from the name of the
         function implementing it.
         """
 
         return self.imp.__name__
 
-    def extract_selection(
-            self,
-            chain_json: Dict,
-            record_json: Dict
-        ) -> Dict[str, Any]:
+    def extract_selection(self, chain_json: Dict,
+                          record_json: Dict) -> Dict[str, Any]:
         """
         Given the `chain` that produced the given `record`, extract from
         `record` the values that will be sent as arguments to the implementation
         as specified by `self.selectors`.
         """
 
         ret = {}
@@ -449,15 +466,17 @@
                 output_key = chain_json['output_keys'][0]
 
                 q = Record.chain._call.rets[output_key]
 
             else:
                 raise RuntimeError(f"Unhandled selection type {type(v)}.")
 
-            val = TruDB.project(query=q, record_json=record_json, chain_json=chain_json)
+            val = TruDB.project(
+                query=q, record_json=record_json, chain_json=chain_json
+            )
             ret[k] = val
 
         return ret
 
 
 pat_1_10 = re.compile(r"\s*([1-9][0-9]*)\s*")
 
@@ -487,14 +506,15 @@
         return cls(**kwargs)
 
     def to_json(self: 'Provider', **extras) -> Dict:
         obj = {'class': self.__class__.__name__}
         obj.update(**extras)
         return obj
 
+
 class OpenAI(Provider):
 
     def __init__(self, model_engine: str = "gpt-3.5-turbo"):
         """
         A set of OpenAI Feedback Functions.
 
         Parameters:
```

## trulens_eval/util.py

```diff
@@ -5,27 +5,73 @@
 """
 
 import logging
 from multiprocessing.pool import AsyncResult
 from multiprocessing.pool import ThreadPool
 from queue import Queue
 from time import sleep
-from typing import Callable, Dict, Hashable, List, Optional, TypeVar
+from typing import Any, Callable, Dict, Hashable, List, Optional, Sequence, TypeVar, Union
 
 from multiprocessing.context import TimeoutError
+from dataclasses import dataclass
 
 import pandas as pd
 from tqdm.auto import tqdm
 
 T = TypeVar("T")
 
 UNICODE_CHECK = "✅"
 UNCIODE_YIELD = "⚡"
 
 
+def first(seq: Sequence[T]) -> T:
+    return seq[0]
+
+
+def second(seq: Sequence[T]) -> T:
+    return seq[1]
+
+
+def third(seq: Sequence[T]) -> T:
+    return seq[2]
+
+
+class JLens(object):
+    # TODO(piotrm): more appropriate version of tinydb.Query
+
+    class Step():
+        pass
+
+    @dataclass
+    class GetAttribute(Step):
+        attribute: str
+
+    @dataclass
+    class GetItem(Step):
+        item: Union[str, int]
+
+    class Aggregate(Step):
+        pass
+
+    def __init__(self):
+        self._path = []
+
+    def __call__(self, json: Dict) -> Union[Any, Sequence[Any], Dict[Any, Any]]:
+        pass
+
+    def _agg(self, aggregator: Callable) -> Any:
+        pass
+
+    def __getitem__(self, index: int) -> 'JLens':
+        pass
+
+    def __getattribute__(self, name: str) -> 'JLens':
+        pass
+
+
 class SingletonPerName():
     """
     Class for creating singleton instances except there being one instance max,
     there is one max per different `name` argument. If `name` is never given,
     reverts to normal singleton behaviour.
     """
 
@@ -57,42 +103,33 @@
 
         # TODO(piotrm): if more tasks than `processes` get added, future ones
         # will block and earlier ones may never start executing.
         self.thread_pool = ThreadPool(processes=1024)
         self.running = 0
         self.promises = Queue(maxsize=1024)
 
-    def _started(self, *args, **kwargs):
-        self.running += 1
-
-    def _finished(self, *args, **kwargs):
-        self.running -= 1
-
     def runrepeatedly(self, func: Callable, rpm: float = 6, *args, **kwargs):
+
         def runner():
             while True:
                 func(*args, **kwargs)
                 sleep(60 / rpm)
 
         self.runlater(runner)
 
     def runlater(self, func: Callable, *args, **kwargs) -> None:
-        self._started()
-
-        prom = self.thread_pool.apply_async(func, callback=self._finished, args=args, kwds=kwargs)
+        prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
         self.promises.put(prom)
 
-    def promise(self, func: Callable[..., T], *args,
-                **kwargs) -> AsyncResult:
-        self._started()
-        prom = self.thread_pool.apply_async(func, callback=self._finished, args=args, kwds=kwargs)
+    def promise(self, func: Callable[..., T], *args, **kwargs) -> AsyncResult:
+        prom = self.thread_pool.apply_async(func, args=args, kwds=kwargs)
         self.promises.put(prom)
 
         return prom
-    
+
     def finish(self, timeout: Optional[float] = None) -> int:
         print(f"Finishing {self.promises.qsize()} task(s) ", end='')
 
         timeouts = []
 
         while not self.promises.empty():
             prom = self.promises.get()
@@ -109,14 +146,14 @@
         if len(timeouts) == 0:
             print("done.")
         else:
             print("some tasks timed out.")
 
         return len(timeouts)
 
-    def status(self) -> List[str]:
+    def _status(self) -> List[str]:
         rows = []
 
         for p in self.thread_pool._pool:
             rows.append([p.is_alive(), str(p)])
-            
+
         return pd.DataFrame(rows, columns=["alive", "thread"])
```

## trulens_eval/pages/Evaluations.py

```diff
@@ -11,14 +11,15 @@
 from ux.add_logo import add_logo
 
 from trulens_eval import Tru
 from trulens_eval import tru_db
 from trulens_eval.tru_db import is_empty
 from trulens_eval.tru_db import is_noserio
 from trulens_eval.tru_db import TruDB
+from trulens_eval.ux.components import render_calls
 
 st.set_page_config(page_title="Evaluations", layout="wide")
 
 st.title("Evaluations")
 
 st.runtime.legacy_caching.clear_cache()
 
@@ -30,16 +31,15 @@
 df_results, feedback_cols = lms.get_records_and_feedback([])
 
 if df_results.empty:
     st.write("No records yet...")
 
 else:
     chains = list(df_results.chain_id.unique())
-
-    if 'Chains' in st.session_state:
+    if 'chain' in st.session_state:
         chain = st.session_state.chain
     else:
         chain = chains
 
     options = st.multiselect('Filter Chains', chains, default=chain)
 
     if (len(options) == 0):
@@ -82,16 +82,20 @@
                     'backgroundColor': 'white'
                 }
             }
         };
         """
         )
 
-        gb.configure_column('record_id', header_name='Record ID')
+        gb.configure_column('record_json', header_name='Record JSON', hide=True)
+        gb.configure_column('chain_json', header_name='Chain JSON', hide=True)
+
+        gb.configure_column('record_id', header_name='Record ID', hide=True)
         gb.configure_column('chain_id', header_name='Chain ID')
+        gb.configure_column('feedback_id', header_name='Feedback ID', hide=True)
         gb.configure_column('input', header_name='User Input')
         gb.configure_column(
             'output',
             header_name='Response',
         )
         gb.configure_column('total_tokens', header_name='Total Tokens')
         gb.configure_column('total_cost', header_name='Total Cost')
@@ -131,14 +135,17 @@
 
             with st.expander("Response", expanded=True):
                 st.write(response)
 
             record_str = selected_rows['record_json'][0]
             record_json = json.loads(record_str)
 
+            st.header("Call Trace")
+            render_calls(record_json)
+
             details = selected_rows['chain_json'][0]
             details_json = json.loads(details)
             #json.loads(details))  # ???
 
             chain_json = details_json['chain']
 
             llm_queries = list(
@@ -174,18 +181,44 @@
                     # CSS to inject contained in a string
                     hide_table_row_index = """
                                 <style>
                                 thead tr th:first-child {display:none}
                                 tbody th {display:none}
                                 </style>
                                 """
-                    df = pd.DataFrame.from_dict(llm_kv, orient='index')
+                    df = pd.DataFrame.from_dict(
+                        llm_kv, orient='index'
+                    ).transpose()
+
+                    # Iterate over each column of the DataFrame
+                    for column in df.columns:
+                        # Check if any cell in the column is a dictionary
+                        if any(isinstance(cell, dict) for cell in df[column]):
+                            # Create new columns for each key in the dictionary
+                            new_columns = df[column].apply(
+                                lambda x: pd.Series(x)
+                                if isinstance(x, dict) else pd.Series()
+                            )
+                            new_columns.columns = [
+                                f"{key}" for key in new_columns.columns
+                            ]
+
+                            # Remove extra zeros after the decimal point
+                            new_columns = new_columns.applymap(
+                                lambda x: '{0:g}'.format(x)
+                                if isinstance(x, float) else x
+                            )
+
+                            # Add the new columns to the original DataFrame
+                            df = pd.concat(
+                                [df.drop(column, axis=1), new_columns], axis=1
+                            )
                     # Inject CSS with Markdown
                     st.markdown(hide_table_row_index, unsafe_allow_html=True)
-                    st.table(df.transpose())
+                    st.table(df)
 
                 if i < len(prompt_queries):
                     query, prompt_details_json = prompt_queries[i]
                     path_str = TruDB._query_str(query)
                     st.subheader(f"Prompt Details:")
 
                     prompt_types = {
```

## trulens_eval/pages/Progress.py

```diff
@@ -11,15 +11,14 @@
 from st_aggrid.shared import JsCode
 import streamlit as st
 from trulens_eval.tru_feedback import Feedback
 from trulens_eval.util import TP
 from ux.add_logo import add_logo
 from trulens_eval.tru_db import is_empty
 
-
 from trulens_eval import tru_db, Tru
 from trulens_eval.provider_apis import Endpoint
 
 from trulens_eval.tru_db import is_noserio
 from trulens_eval.tru_db import TruDB
 
 st.set_page_config(page_title="Feedback Progress", layout="wide")
@@ -38,15 +37,15 @@
 e_cohere = Endpoint("cohere")
 
 endpoints = [e_openai, e_hugs, e_cohere]
 
 tab1, tab2, tab3 = st.tabs(["Progress", "Endpoints", "Feedback Functions"])
 
 with tab1:
-    feedbacks = lms.get_feedback(status=[-1,0,1])
+    feedbacks = lms.get_feedback(status=[-1, 0, 1])
     st.write(feedbacks)
 
 with tab2:
     for e in endpoints:
         st.header(e.name.upper())
         st.metric("RPM", e.rpm)
         st.write(e.tqdm)
```

## trulens_eval/ux/add_logo.py

```diff
@@ -2,17 +2,16 @@
 
 import pkg_resources
 import streamlit as st
 
 
 def add_logo():
     logo = open(
-        pkg_resources.resource_filename(
-            'trulens_eval', 'ux/trulens_logo.svg'
-        ), "rb"
+        pkg_resources.resource_filename('trulens_eval', 'ux/trulens_logo.svg'),
+        "rb"
     ).read()
 
     logo_encoded = base64.b64encode(logo).decode()
     st.markdown(
         f"""
         <style>
             [data-testid="stSidebarNav"] {{
```

## Comparing `trulens_eval-0.1.1a0.dist-info/METADATA` & `trulens_eval-0.1.2a0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.1.1a0
+Version: 0.1.2a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
@@ -28,14 +28,15 @@
 Requires-Dist: streamlit (>=1.22.0)
 Requires-Dist: streamlit-aggrid (>=0.3.4.post3)
 Requires-Dist: streamlit-extras (>=0.2.7)
 Requires-Dist: tinydb (>=4.7.1)
 Requires-Dist: transformers (>=4.10.0)
 Requires-Dist: typing-inspect (==0.8.0)
 Requires-Dist: typing-extensions (==4.5.0)
+Requires-Dist: frozendict (>=2.3.8)
 
 # Welcome to TruLens-Eval!
 
 ![TruLens](https://www.trulens.org/Assets/image/Neural_Network_Explainability.png)
 
 Evaluate and track your LLM experiments with TruLens. As you work on your models and prompts TruLens-Eval supports the iterative development and of a wide range of LLM applications by wrapping your application to log key metadata across the entire chain (or off chain if your project does not use chains) on your local machine.
```

## Comparing `trulens_eval-0.1.1a0.dist-info/RECORD` & `trulens_eval-0.1.2a0.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,28 @@
 trulens_eval/Example_TruBot.py,sha256=vUPkVRoR0DBTIyp7UifHP6TdEQU0ppI6YGFAfLOulHw,4782
 trulens_eval/Leaderboard.py,sha256=AEPQhBQkqG11WqcQoxtyGIaGkXsNIBrsRBlu9nL8pGw,2422
-trulens_eval/__init__.py,sha256=ZqDLAgPzM_bMOkrcMNqTelU0_7jBWds2z1052PwTzWg,407
+trulens_eval/__init__.py,sha256=ZOO2u2rZt-FKf4P7sPppGUmSzekw09NF5OZPCeYwvuk,407
 trulens_eval/benchmark.py,sha256=LHVnqYTudYpOJ2iry7De41jLfO2FImZqprUlaUkOiKA,5401
 trulens_eval/feedback_prompts.py,sha256=DgW4_f_4g018tYNwca1D1taJhhdwaf2fDR9J8s0Upls,3443
 trulens_eval/keys.py,sha256=5HwSGVImj1742PRYo0iV9SC_92VO-3B3f7vH8503fd4,949
-trulens_eval/provider_apis.py,sha256=vSEzQmPvE2OuJvHZLqaqB_BLXyWODKd1PrZSY1wUVyk,3325
-trulens_eval/slackbot.py,sha256=L8rJnTapQV_gl_eCJFL-5SauS2zSWM4lMWggJ8g7AS4,12097
+trulens_eval/provider_apis.py,sha256=Ed9-qEAJywdXage73NZIhuBBUAOrTqhLzMqIPvbWvuI,3341
+trulens_eval/slackbot.py,sha256=u180sBMLVLgI0KW3oQI8P3Tz5p89_-zcxVU7NqzWCtQ,11179
 trulens_eval/test_tru_chain.py,sha256=fEpZzCB1OaCfQ3AVe7yjShxxxFJrmR9E660o_47yZIU,5455
-trulens_eval/tru.py,sha256=gq6QVKkapVB21jTarU0uKZCo8Y4ldwDnk2yDMrRnIaA,10400
-trulens_eval/tru_chain.py,sha256=T4v8Uli-bAYTFe5nXXHF3lMfhBWH99XYGRkIVgVpgbI,21491
-trulens_eval/tru_db.py,sha256=YHQDvvSW550dpftTBfH31-w1iXEdTYTO3EgtRwbkCRo,27984
-trulens_eval/tru_feedback.py,sha256=yNCBzvsVUPqyaRsLzFqZn8IWPKrVep-IgZWZUfPAHgw,30260
-trulens_eval/util.py,sha256=0Ih7QhEcCz8cRfb9idaFc-nHe7cfop65KByIkcbfO84,3478
-trulens_eval/pages/Evaluations.py,sha256=6zxLRBVX4-iPxTkTa7LhPAGMz6e3hth3jmtmzwDA-9o,8618
-trulens_eval/pages/Progress.py,sha256=YztP-JVWCgdiuT5Qu8TCXQGliWOfGpx6DuAoyyeh0ZY,1322
+trulens_eval/tru.py,sha256=IjwAsyOXkEFONizvRoHzygwW_G8HeN3gqUKrqSMI6Vo,10449
+trulens_eval/tru_chain.py,sha256=8oHTL2p4UdBCDvkVxA2-QflOzzgeS44gtJ2o3AIZWMI,22796
+trulens_eval/tru_db.py,sha256=u-3Dyo9XggCpwJLS4rU0alCRIJC980uGuGp80aUWf4g,29397
+trulens_eval/tru_feedback.py,sha256=yiX7DSlOquQ34rZIGoLhZ7u_pxbCe58tDCjdJZCrp9g,30628
+trulens_eval/util.py,sha256=-K_kx2jsYUPQb7Ar63qZlxOJptYFrdAw_lLDXn4Gw1c,4067
+trulens_eval/examples/App_TruBot.py,sha256=vUPkVRoR0DBTIyp7UifHP6TdEQU0ppI6YGFAfLOulHw,4782
+trulens_eval/examples/trubot.py,sha256=kaI-FR5hqhtTx9fd2hyQjbTGzEedM0Ic4eSfMSyrnqc,11074
+trulens_eval/pages/Evaluations.py,sha256=YGqYck7esPDCfU26aQbRne7kRs1_6vE1F62bpzMxEGs,10278
+trulens_eval/pages/Progress.py,sha256=w_CF6QaKBI-6_kFIVVX-bNimtXbGB5UUd6xACfIMty4,1323
 trulens_eval/pages/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-trulens_eval/ux/add_logo.py,sha256=nlK4cdfbRiasTkMiP14akmvtxhSy7GLb3Wb_vd0m8GA,929
+trulens_eval/tests/test_tru_chain.py,sha256=fEpZzCB1OaCfQ3AVe7yjShxxxFJrmR9E660o_47yZIU,5455
+trulens_eval/utils/langchain.py,sha256=NocH5hnzxkBRmT_TxtOBh7cI6WmKLZUVJSJDnza0dnw,1484
+trulens_eval/ux/add_logo.py,sha256=Pwl6mfzAX1VSi2VTOZsMBoCstaBuVGoSrs7P5tomP4U,915
+trulens_eval/ux/components.py,sha256=WanlzqME2NSQ4KnLGx7Y9gglhEhpgC5mj5Hvw66HysY,1142
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.1.1a0.dist-info/METADATA,sha256=tcAdeGJ_S6rq0LjKMEiETGJN2nkZ_TzgyMtfZ9sWpZw,12698
-trulens_eval-0.1.1a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-trulens_eval-0.1.1a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.1.1a0.dist-info/RECORD,,
+trulens_eval-0.1.2a0.dist-info/METADATA,sha256=ub40Py0dGdBwJ_Jzod-mwzvKsLgG5pWs1RnnejIWs88,12734
+trulens_eval-0.1.2a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.1.2a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.1.2a0.dist-info/RECORD,,
```

